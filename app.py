"""
ğŸ¤ Worship Vocal AI Coach
===========================

í†µí•© Streamlit ì•± - ë‹¨ì¼ ë¶„ì„ + ì´ì¤‘ ë¶„ì„(Dual-Core) ì§€ì›

ì‚¬ìš©ë²• (ë¡œì»¬ì—ì„œ):
    source venv/bin/activate
    streamlit run app.py
"""

import streamlit as st
import os
import tempfile
from pathlib import Path
from datetime import datetime
import numpy as np
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import librosa

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="Worship Vocal AI Coach",
    page_icon="ğŸ¤",
    layout="wide"
)

# =============================================
# Premium UI ìŠ¤íƒ€ì¼ ì ìš©
# =============================================
from components.styles import inject_custom_css
from components.charts import CHART_THEME, get_premium_layout, style_radar_chart, style_bar_chart, style_line_chart, style_histogram

inject_custom_css()

# =============================================
# P0 UX ê°œì„ : ì „ë¬¸ ìš©ì–´ ë²ˆì—­ ì‹œìŠ¤í…œ
# =============================================
TERM_TRANSLATIONS = {
    "Spectral Centroid": "ìŒìƒ‰ ë°ê¸°",
    "Dynamic Range": "ê°•ì•½ í‘œí˜„ í­",
    "Pitch Accuracy": "ìŒì • ì •í™•ë„",
    "Pitch Stability": "ìŒ ì•ˆì •ì„±",
    "RMS": "í‰ê·  ìŒëŸ‰",
    "Vibrato Ratio": "ë–¨ë¦¼ ì •ë„",
    "Breath Support": "í˜¸í¡ ì§€ì§€ë ¥",
    "High Note Stability": "ê³ ìŒ ì•ˆì •ì„±",
    "Articulation": "ë°œìŒ ëª…ë£Œë„",
    "Rhythm Offset": "ë¦¬ë“¬ ì •í™•ë„",
}

TERM_HELP = {
    "ìŒìƒ‰ ë°ê¸°": "ìŒìƒ‰ì´ ë°ì€ì§€ ë”°ëœ»í•œì§€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ë‚®ì„ìˆ˜ë¡ ë”°ëœ»í•˜ê³ , ë†’ì„ìˆ˜ë¡ ë°ìŠµë‹ˆë‹¤.",
    "ê°•ì•½ í‘œí˜„ í­": "ê°€ì¥ ì‘ì€ ì†Œë¦¬ì™€ ê°€ì¥ í° ì†Œë¦¬ì˜ ì°¨ì´ì…ë‹ˆë‹¤. ë„“ì„ìˆ˜ë¡ í‘œí˜„ë ¥ì´ í’ë¶€í•©ë‹ˆë‹¤.",
    "ìŒì • ì •í™•ë„": "ëª©í‘œ ìŒì •ê³¼ì˜ ì°¨ì´ì…ë‹ˆë‹¤. ë‚®ì„ìˆ˜ë¡ ì •í™•í•©ë‹ˆë‹¤. (ë‹¨ìœ„: cents, 100 cents = ë°˜ìŒ)",
    "ìŒ ì•ˆì •ì„±": "ìŒì„ ìœ ì§€í•  ë•Œ ì–¼ë§ˆë‚˜ í”ë“¤ë¦¬ì§€ ì•ŠëŠ”ì§€ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.",
    "í‰ê·  ìŒëŸ‰": "ì „ì²´ì ì¸ ì†Œë¦¬ í¬ê¸°ì…ë‹ˆë‹¤.",
    "ë–¨ë¦¼ ì •ë„": "ë¹„ë¸Œë¼í† (ì˜ë„ì  ë–¨ë¦¼)ì˜ ì •ë„ì…ë‹ˆë‹¤.",
    "í˜¸í¡ ì§€ì§€ë ¥": "í•œ í˜¸í¡ìœ¼ë¡œ ì–¼ë§ˆë‚˜ ê¸¸ê²Œ ë…¸ë˜í•  ìˆ˜ ìˆëŠ”ì§€ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.",
    "ê³ ìŒ ì•ˆì •ì„±": "ë†’ì€ ìŒì—ì„œ ì–¼ë§ˆë‚˜ ì•ˆì •ì ìœ¼ë¡œ ì†Œë¦¬ë¥¼ ìœ ì§€í•˜ëŠ”ì§€ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.",
    "ë°œìŒ ëª…ë£Œë„": "ê°€ì‚¬ê°€ ì–¼ë§ˆë‚˜ ë˜ë ·í•˜ê²Œ ì „ë‹¬ë˜ëŠ”ì§€ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.",
    "ë¦¬ë“¬ ì •í™•ë„": "ë°•ìì— ì–¼ë§ˆë‚˜ ì •í™•í•˜ê²Œ ë§ì¶”ëŠ”ì§€ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.",
}

# P0 UX ê°œì„ : ë°±ë¶„ìœ„ ê¸°ì¤€ (í†µê³„ ê¸°ë°˜)
PERCENTILE_THRESHOLDS = {
    "pitch_accuracy_cents": [
        (0, 15, "ìƒìœ„ 10%", "delta_good"),
        (15, 25, "ìƒìœ„ 30%", "delta_good"),
        (25, 35, "í‰ê· ", "delta_neutral"),
        (35, 100, "ì—°ìŠµ í•„ìš”", "delta_bad"),
    ],
    "high_note_stability": [
        (80, 101, "ìƒìœ„ 10%", "delta_good"),
        (60, 80, "ìƒìœ„ 30%", "delta_good"),
        (40, 60, "í‰ê· ", "delta_neutral"),
        (0, 40, "ì—°ìŠµ í•„ìš”", "delta_bad"),
    ],
    "dynamic_range": [
        (18, 100, "ìƒìœ„ 10%", "delta_good"),
        (14, 18, "ìƒìœ„ 30%", "delta_good"),
        (10, 14, "í‰ê· ", "delta_neutral"),
        (0, 10, "í‘œí˜„ë ¥ ë¶€ì¡±", "delta_bad"),
    ],
}

def get_percentile_badge(metric: str, value: float) -> tuple:
    """ì ìˆ˜ì— ëŒ€í•œ ë°±ë¶„ìœ„ ë±ƒì§€ ë°˜í™˜"""
    thresholds = PERCENTILE_THRESHOLDS.get(metric, [])
    for min_v, max_v, label, delta_type in thresholds:
        if min_v <= value < max_v:
            return label, delta_type
    return "", "delta_neutral"

# P0 UX ê°œì„ : ë¶„ì„ ì§„í–‰ ë‹¨ê³„
ANALYSIS_STEPS = [
    ("ğŸµ ì˜¤ë””ì˜¤ ë¡œë”© ì¤‘...", "íŒŒì¼ì„ ì½ê³  ìˆì–´ìš”", 5),
    ("ğŸ­ ë³´ì»¬ ë¶„ë¦¬ ì¤‘...", "AIê°€ ëª©ì†Œë¦¬ë§Œ ì¶”ì¶œí•˜ê³  ìˆì–´ìš” (1-3ë¶„)", 25),
    ("ğŸ“Š ìŒì„± ë¶„ì„ ì¤‘...", "í”¼ì¹˜, ìŒëŸ‰, ìŒìƒ‰ì„ ë¶„ì„í•˜ê³  ìˆì–´ìš”", 55),
    ("ğŸ§¬ ë³´ì»¬ DNA ê³„ì‚° ì¤‘...", "ë‹¹ì‹ ì˜ ë³´ì»¬ ìŠ¤íƒ€ì¼ì„ íŒŒì•…í•˜ê³  ìˆì–´ìš”", 75),
    ("ğŸ¤– AI ì½”ì¹­ ìƒì„± ì¤‘...", "ë§ì¶¤ í”¼ë“œë°±ì„ ì‘ì„±í•˜ê³  ìˆì–´ìš”", 90),
    ("âœ… ì™„ë£Œ!", "ë¶„ì„ì´ ëë‚¬ì–´ìš”!", 100),
]

# =============================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# =============================================

def time_to_seconds(t: str) -> int:
    """ì‹œê°„ ë¬¸ìì—´ì„ ì´ˆë¡œ ë³€í™˜"""
    if not t:
        return None
    parts = t.split(":")
    if len(parts) == 2:
        return int(parts[0]) * 60 + int(parts[1])
    elif len(parts) == 3:
        return int(parts[0]) * 3600 + int(parts[1]) * 60 + int(parts[2])
    return int(t)


def sanitize_filename(title: str, max_length: int = 50) -> str:
    """íŒŒì¼ëª…ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ë¬¸ìì—´ ì •ë¦¬"""
    import re
    # íŠ¹ìˆ˜ë¬¸ì ì œê±° (í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê³µë°±, í•˜ì´í”ˆ, ì–¸ë”ìŠ¤ì½”ì–´ë§Œ í—ˆìš©)
    sanitized = re.sub(r'[^\w\sê°€-í£-]', '', title)
    # ì—°ì† ê³µë°±ì„ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ
    sanitized = re.sub(r'\s+', '_', sanitized.strip())
    # ê¸¸ì´ ì œí•œ
    if len(sanitized) > max_length:
        sanitized = sanitized[:max_length]
    return sanitized or "untitled"


def extract_youtube_audio(url: str, start_time: str, end_time: str, output_name: str = None) -> tuple:
    """YouTubeì—ì„œ ì˜¤ë””ì˜¤ ì¶”ì¶œ (pytubefix ì‚¬ìš©)

    Returns:
        tuple: (ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ, ì˜ìƒ ì œëª©)
    """
    import subprocess
    from pytubefix import YouTube

    # pytubefixë¡œ ì˜ìƒ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    yt = YouTube(url)
    video_title = yt.title or "untitled"

    # ì˜ìƒ ì œëª©ì„ íŒŒì¼ëª…ìœ¼ë¡œ ì‚¬ìš© (output_nameì´ ì—†ê±°ë‚˜ genericí•œ ê²½ìš°)
    if not output_name or output_name in ["mission_a", "mission_b", "single", "single_analysis"]:
        safe_name = sanitize_filename(video_title)
    else:
        safe_name = output_name

    output_path = f"/tmp/{safe_name}.mp3"

    audio_stream = yt.streams.get_audio_only()

    if not audio_stream:
        raise Exception("ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

    downloaded_file = audio_stream.download(output_path='/tmp', filename=f"{safe_name}_full")

    # êµ¬ê°„ ì¶”ì¶œ (ffmpeg ì‚¬ìš©)
    start_sec = time_to_seconds(start_time)
    end_sec = time_to_seconds(end_time) if end_time else None

    if start_sec and end_sec and end_sec > start_sec:
        duration = end_sec - start_sec
        cmd = ["ffmpeg", "-y", "-i", downloaded_file, "-ss", str(start_sec),
               "-t", str(duration), "-vn", "-acodec", "libmp3lame", "-q:a", "2", output_path]
        subprocess.run(cmd, check=True, capture_output=True)
    else:
        # ì „ì²´ íŒŒì¼ì„ mp3ë¡œ ë³€í™˜
        cmd = ["ffmpeg", "-y", "-i", downloaded_file, "-vn", "-acodec", "libmp3lame", "-q:a", "2", output_path]
        subprocess.run(cmd, check=True, capture_output=True)

    return output_path, video_title


def analyze_audio_features(audio_path: str, include_timeseries: bool = False) -> dict:
    """ì˜¤ë””ì˜¤ íŒŒì¼ì—ì„œ íŠ¹ì§• ì¶”ì¶œ

    Args:
        audio_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        include_timeseries: Trueë©´ ì‹œê³„ì—´ ë°ì´í„°ë„ í¬í•¨ (ì°¨íŠ¸ìš©)
    """
    y, sr = librosa.load(audio_path, sr=22050)
    duration = len(y) / sr

    # í”¼ì¹˜ ì¶”ì¶œ
    f0, voiced_flag, voiced_probs = librosa.pyin(y, fmin=80, fmax=800, sr=sr)
    valid_f0 = f0[~np.isnan(f0)]

    # í”¼ì¹˜ ì‹œê°„ì¶•
    hop_length = 512
    times = librosa.times_like(f0, sr=sr, hop_length=hop_length)

    # RMS (ë³¼ë¥¨)
    rms = librosa.feature.rms(y=y)[0]
    rms_db = librosa.amplitude_to_db(rms + 1e-10)
    rms_times = librosa.times_like(rms, sr=sr, hop_length=hop_length)

    # ìŠ¤í™íŠ¸ëŸ¼
    centroid = librosa.feature.spectral_centroid(y=y, sr=sr)[0]
    centroid_times = librosa.times_like(centroid, sr=sr, hop_length=hop_length)

    # Zero Crossing Rate
    zcr = librosa.feature.zero_crossing_rate(y)[0]
    zcr_times = librosa.times_like(zcr, sr=sr, hop_length=hop_length)

    # í…œí¬ ë° ë¹„íŠ¸ ì¶”ì¶œ
    tempo, beats = librosa.beat.beat_track(y=y, sr=sr)
    if isinstance(tempo, np.ndarray):
        tempo = float(tempo[0])
    beat_times = librosa.frames_to_time(beats, sr=sr, hop_length=hop_length)

    # ë¦¬ë“¬ ì˜¤í”„ì…‹ ì‹¤ì¸¡ (onset-beat ë™ê¸°í™”)
    onset_env = librosa.onset.onset_strength(y=y, sr=sr)
    onset_frames = librosa.onset.onset_detect(onset_envelope=onset_env, sr=sr)
    onset_times = librosa.frames_to_time(onset_frames, sr=sr, hop_length=hop_length)

    # ê° onsetì´ ê°€ì¥ ê°€ê¹Œìš´ beatì™€ ì–¼ë§ˆë‚˜ ë–¨ì–´ì ¸ ìˆëŠ”ì§€ ê³„ì‚°
    rhythm_offsets = []
    if len(beat_times) > 0 and len(onset_times) > 0:
        for onset in onset_times:
            closest_beat_idx = np.argmin(np.abs(beat_times - onset))
            offset_ms = abs(onset - beat_times[closest_beat_idx]) * 1000  # ms
            rhythm_offsets.append(offset_ms)
        rhythm_offset_ms = float(np.mean(rhythm_offsets)) if rhythm_offsets else 50.0
    else:
        rhythm_offset_ms = 50.0  # ë¹„íŠ¸/ì˜¨ì…‹ ì—†ìœ¼ë©´ ì¤‘ë¦½ê°’

    # í”¼ì¹˜ í†µê³„
    if len(valid_f0) > 0:
        pitch_mean = np.mean(valid_f0)
        pitch_std = np.std(valid_f0)
        pitch_min = np.min(valid_f0)
        pitch_max = np.max(valid_f0)
        pitch_range = librosa.hz_to_midi(pitch_max) - librosa.hz_to_midi(pitch_min)

        # ìŒì • ì •í™•ë„ (cents)
        midi_notes = librosa.hz_to_midi(valid_f0)
        rounded_midi = np.round(midi_notes)
        pitch_errors = (midi_notes - rounded_midi) * 100  # cents
        pitch_accuracy = np.mean(np.abs(pitch_errors))

        # ê²½í–¥
        flat_ratio = np.sum(pitch_errors < -10) / len(pitch_errors)
        sharp_ratio = np.sum(pitch_errors > 10) / len(pitch_errors)

        # ê³ /ì €ìŒ ë¹„ìœ¨
        high_threshold = np.percentile(valid_f0, 75)
        high_ratio = np.sum(valid_f0 > high_threshold) / len(valid_f0)
        low_threshold = np.percentile(valid_f0, 25)
        low_ratio = np.sum(valid_f0 < low_threshold) / len(valid_f0)

        # ê³ ìŒ ì•ˆì •ì„± (ì„¼íŠ¸ ê¸°ë°˜ - Hz ê¸°ë°˜ë³´ë‹¤ ì •í™•)
        high_notes = valid_f0[valid_f0 > high_threshold]
        if len(high_notes) > 10:
            # Hzë¥¼ MIDI (ë°˜ìŒ ë‹¨ìœ„)ë¡œ ë³€í™˜ í›„ í‘œì¤€í¸ì°¨ ê³„ì‚°
            high_notes_midi = librosa.hz_to_midi(high_notes)
            high_notes_std_semitones = np.std(high_notes_midi)
            # ìˆ˜ì •ëœ ê¸°ì¤€: 4.0 ë°˜ìŒ (ë” ê´€ëŒ€), ìµœì†Œ 20%
            # 0.5 ë°˜ìŒ std â†’ 87%, 1.5 ë°˜ìŒ â†’ 62%, 3.0 ë°˜ìŒ â†’ 25%, 4.0+ ë°˜ìŒ â†’ 20%
            raw_stability = 1 - (high_notes_std_semitones / 4.0)
            high_note_stability = max(0.2, min(1, raw_stability))  # ìµœì†Œ 20% ë³´ì¥
        else:
            high_note_stability = 0.6  # ë°ì´í„° ë¶€ì¡±ì‹œ ì¤‘ë¦½ê°’ (ì•½ê°„ ë†’ê²Œ)
    else:
        pitch_mean = 200
        pitch_std = 50
        pitch_min = 100
        pitch_max = 400
        pitch_range = 20
        pitch_accuracy = 30
        pitch_errors = np.array([0])
        flat_ratio = 0.3
        sharp_ratio = 0.3
        high_ratio = 0.2
        low_ratio = 0.2
        high_note_stability = 0.7
        high_threshold = 300
        low_threshold = 150

    # ë‹¤ì´ë‚˜ë¯¹ ë ˆì¸ì§€
    dynamic_range = np.max(rms_db) - np.percentile(rms_db, 10)

    # ë‹¤ì´ë‚˜ë¯¹ ì ìˆ˜ (ì „ë¬¸ê°€ íŒ¨ë„ ê¶Œì¥: 12-20dBê°€ ìµœì )
    if dynamic_range < 12:
        dynamic_score = (dynamic_range / 12) * 0.5  # 0-50%
    elif dynamic_range <= 22:
        dynamic_score = 0.5 + ((dynamic_range - 12) / 10) * 0.5  # 50-100%
    else:
        dynamic_score = max(0.6, 1.0 - (dynamic_range - 22) * 0.02)  # 100ì—ì„œ ê°ì†Œ

    # ë¹„ë¸Œë¼í†  ë¶„ì„ (ì£¼ê¸°ì„± ê²€ì¶œë¡œ ì˜ë„ì  ë¹„ë¸Œë¼í†  vs ë¶ˆì•ˆì • êµ¬ë¶„)
    vibrato_rate = 0.0  # ë¹„ë¸Œë¼í†  ì£¼íŒŒìˆ˜ (Hz)
    vibrato_depth = 0.0  # ë¹„ë¸Œë¼í†  ê¹Šì´ (ë°˜ìŒ)
    vibrato_regularity = 0.0  # ë¹„ë¸Œë¼í†  ê·œì¹™ì„± (0-1)
    is_intentional_vibrato = False

    if len(valid_f0) > 50:
        # í”¼ì¹˜ë¥¼ ì„¼íŠ¸(cents)ë¡œ ë³€í™˜ (ìŒì•…ì  ë‹¨ìœ„)
        f0_cents = 1200 * np.log2(valid_f0 / (pitch_mean + 1e-6))

        # ìê¸°ìƒê´€(autocorrelation)ìœ¼ë¡œ ì£¼ê¸°ì„± ê²€ì¶œ
        f0_centered = f0_cents - np.mean(f0_cents)
        autocorr = np.correlate(f0_centered, f0_centered, mode='full')
        autocorr = autocorr[len(autocorr)//2:]  # ì–‘ì˜ lagë§Œ
        autocorr = autocorr / (autocorr[0] + 1e-10)  # ì •ê·œí™”

        # ë¹„ë¸Œë¼í†  ì£¼íŒŒìˆ˜ ë²”ìœ„: 4-8 Hz (ì¼ë°˜ì  ë¹„ë¸Œë¼í†  ë²”ìœ„)
        # hop_length=512, sr=22050 â†’ ì•½ 43 frames/sec
        frames_per_sec = sr / hop_length
        min_lag = int(frames_per_sec / 8)  # 8 Hz
        max_lag = int(frames_per_sec / 4)  # 4 Hz

        if max_lag < len(autocorr) and min_lag > 0:
            # ë¹„ë¸Œë¼í†  ë²”ìœ„ì—ì„œ í”¼í¬ ì°¾ê¸°
            vibrato_region = autocorr[min_lag:max_lag]
            if len(vibrato_region) > 0:
                peak_idx = np.argmax(vibrato_region)
                peak_value = vibrato_region[peak_idx]

                # ë¹„ë¸Œë¼í†  íŒì •: ìê¸°ìƒê´€ í”¼í¬ê°€ 0.3 ì´ìƒì´ë©´ ì£¼ê¸°ì 
                if peak_value > 0.3:
                    actual_lag = min_lag + peak_idx
                    vibrato_rate = frames_per_sec / actual_lag  # Hz
                    vibrato_depth = np.std(f0_cents) / 100  # ë°˜ìŒ ë‹¨ìœ„
                    vibrato_regularity = float(peak_value)
                    is_intentional_vibrato = True

    # ë¹„ë¸Œë¼í†  ë¹„ìœ¨ (í•˜ìœ„ í˜¸í™˜ì„±)
    if is_intentional_vibrato:
        # ì˜ë„ì  ë¹„ë¸Œë¼í† : ê¹Šì´ì™€ ê·œì¹™ì„± ê¸°ë°˜
        vibrato_ratio = min(1.0, vibrato_depth * vibrato_regularity * 2)
    else:
        # ë¶ˆì•ˆì •í•œ í”¼ì¹˜ ë³€ë™
        vibrato_ratio = pitch_std / (pitch_mean + 1e-6)
        vibrato_ratio = min(1.0, vibrato_ratio * 10)

    # ë°œìŒ ì„ ëª…ë„ ê°œì„  (spectral flux + centroid ê²°í•©)
    # Spectral flux (ìŠ¤í™íŠ¸ëŸ¼ ë³€í™”ìœ¨) - ë°œìŒì´ ë˜ë ·í• ìˆ˜ë¡ ë†’ìŒ
    S = np.abs(librosa.stft(y, hop_length=hop_length))
    spectral_flux = np.mean(np.diff(S, axis=1) ** 2)
    flux_normalized = min(1.0, spectral_flux / 0.1)

    # Spectral centroid ì ìˆ˜ (1500-2500Hzê°€ ìµœì , ë„ˆë¬´ ë‚®ê±°ë‚˜ ë†’ìœ¼ë©´ ê°ì )
    mean_centroid = np.mean(centroid)
    centroid_score = max(0, 1 - abs(mean_centroid - 2000) / 2000)

    # ê²°í•©: centroid 60% + flux 40%
    articulation_clarity = centroid_score * 0.6 + flux_normalized * 0.4

    # í”„ë ˆì´ì¦ˆ ê¸¸ì´ ì‹¤ì¸¡ (RMS ê¸°ë°˜)
    rms_threshold = np.percentile(rms_db, 25)  # í•˜ìœ„ 25%ë¥¼ 'ì‰¬ëŠ” êµ¬ê°„'
    phrase_lengths = []
    current_length = 0
    for db_val in rms_db:
        if db_val > rms_threshold:
            current_length += hop_length / sr
        elif current_length > 0.5:  # 0.5ì´ˆ ì´ìƒ ìœ íš¨ í”„ë ˆì´ì¦ˆ
            phrase_lengths.append(current_length)
            current_length = 0
        else:
            current_length = 0
    if current_length > 0.5:
        phrase_lengths.append(current_length)
    phrase_length = np.mean(phrase_lengths) if phrase_lengths else 3.0

    # í˜¸í¡ ì§€ì§€ ì ìˆ˜ (4ì´ˆ=50%, 8ì´ˆ=100%)
    breath_support_score = min(1.0, max(0, (phrase_length - 2) / 6))

    result = {
        'duration': duration,
        'avg_pitch_hz': pitch_mean,
        'pitch_min_hz': pitch_min,
        'pitch_max_hz': pitch_max,
        'pitch_std': pitch_std,
        'pitch_range_semitones': pitch_range,
        'pitch_accuracy_cents': pitch_accuracy,
        'pitch_stability': 1 - (pitch_std / (pitch_mean + 1e-6)),
        'high_note_ratio': high_ratio,
        'low_note_ratio': low_ratio,
        'high_note_stability': high_note_stability,
        'high_threshold_hz': high_threshold if len(valid_f0) > 0 else 300,
        'low_threshold_hz': low_threshold if len(valid_f0) > 0 else 150,
        'dynamic_range_db': dynamic_range,
        'dynamic_score': dynamic_score,  # 0-1, ë‹¤ì´ë‚˜ë¯¹ ì ìˆ˜ (ìµœì  ë²”ìœ„ ë°˜ì˜)
        'rms_db_max': np.max(rms_db),
        'rms_db_mean': np.mean(rms_db),
        'energy_variance': np.std(rms),
        'climax_intensity': np.max(rms) / (np.mean(rms) + 1e-6),
        'spectral_centroid_hz': np.mean(centroid),
        'warmth_score': 1 - (np.mean(centroid) / 3000),
        'vibrato_ratio': vibrato_ratio,
        'vibrato_rate_hz': vibrato_rate,  # ë¹„ë¸Œë¼í†  ì£¼íŒŒìˆ˜ (4-8 Hzê°€ ìì—°ìŠ¤ëŸ¬ì›€)
        'vibrato_depth_semitones': vibrato_depth,  # ë¹„ë¸Œë¼í†  ê¹Šì´ (ë°˜ìŒ)
        'vibrato_regularity': vibrato_regularity,  # ê·œì¹™ì„± (0-1, ë†’ì„ìˆ˜ë¡ ì˜ë„ì )
        'is_intentional_vibrato': is_intentional_vibrato,  # ì˜ë„ì  ë¹„ë¸Œë¼í†  ì—¬ë¶€
        'tempo_bpm': tempo,
        'rhythm_offset_ms': rhythm_offset_ms,  # onset-beat ë™ê¸°í™” ì¸¡ì •
        'breath_phrase_length': phrase_length,
        'breath_support_score': breath_support_score,  # 0-1, í˜¸í¡ ì§€ì§€ ì ìˆ˜
        'articulation_clarity': articulation_clarity,
        'flat_tendency': flat_ratio,
        'sharp_tendency': sharp_ratio,
        'rms_mean': np.mean(rms),
        'voiced_ratio': len(valid_f0) / len(f0) if len(f0) > 0 else 0.7,
        'sample_rate': sr
    }

    # ì‹œê³„ì—´ ë°ì´í„° (ì°¨íŠ¸ìš©)
    if include_timeseries:
        result['timeseries'] = {
            'waveform': y,
            'f0': f0,
            'f0_times': times,
            'valid_f0': valid_f0,
            'pitch_errors': pitch_errors,
            'rms': rms,
            'rms_db': rms_db,
            'rms_times': rms_times,
            'centroid': centroid,
            'centroid_times': centroid_times,
            'zcr': zcr,
            'zcr_times': zcr_times
        }

    return result


def create_radar_chart(stats: dict, title: str = "ë³´ì»¬ ìŠ¤íƒ¯ ë ˆì´ë”") -> go.Figure:
    """5ê°í˜• ë ˆì´ë” ì°¨íŠ¸ ìƒì„±"""
    categories = list(stats.keys())
    values = list(stats.values())

    # ë‹«íŒ ë‹¤ê°í˜•ì„ ìœ„í•´ ì²« ë²ˆì§¸ ê°’ ì¶”ê°€
    categories.append(categories[0])
    values.append(values[0])

    fig = go.Figure()

    fig.add_trace(go.Scatterpolar(
        r=values,
        theta=categories,
        fill='toself',
        fillcolor='rgba(201, 169, 98, 0.2)',
        line=dict(color=CHART_THEME["colors"]["gold"], width=2.5),
        marker=dict(
            size=8,
            color=CHART_THEME["colors"]["gold"],
            line=dict(color=CHART_THEME["backgrounds"]["paper"], width=2)
        ),
        name='í˜„ì¬ ìŠ¤íƒ¯',
        hovertemplate='%{theta}: %{r:.0f}<extra></extra>'
    ))

    fig.update_layout(
        polar=dict(
            bgcolor=CHART_THEME["backgrounds"]["plot"],
            radialaxis=dict(
                visible=True,
                range=[0, 100],
                tickfont=dict(color=CHART_THEME["text"]["muted"], size=10),
                gridcolor=CHART_THEME["backgrounds"]["grid"],
                linecolor=CHART_THEME["backgrounds"]["grid"],
            ),
            angularaxis=dict(
                tickfont=dict(color=CHART_THEME["text"]["primary"], size=12),
                gridcolor=CHART_THEME["backgrounds"]["grid"],
                linecolor=CHART_THEME["backgrounds"]["grid"],
            ),
        ),
        paper_bgcolor=CHART_THEME["backgrounds"]["paper"],
        showlegend=False,
        title=dict(
            text=title,
            x=0.5,
            font=dict(size=16, color=CHART_THEME["text"]["primary"])
        ),
        height=400,
        margin=dict(l=80, r=80, t=60, b=40)
    )

    return fig


def create_dna_chart(dna: dict) -> go.Figure:
    """6ì°¨ì› DNA ì°¨íŠ¸ ìƒì„±"""
    categories = list(dna.keys())
    values = list(dna.values())

    # í”„ë¦¬ë¯¸ì—„ ìƒ‰ìƒ íŒ”ë ˆíŠ¸
    colors = [
        CHART_THEME["colors"]["gold"],
        CHART_THEME["colors"]["purple"],
        CHART_THEME["colors"]["success"],
        CHART_THEME["colors"]["info"],
        CHART_THEME["colors"]["pink"],
        CHART_THEME["colors"]["warning"]
    ]

    fig = go.Figure()

    fig.add_trace(go.Bar(
        x=categories,
        y=values,
        marker=dict(color=colors, line=dict(width=0)),
        text=[f'{v:.0f}' for v in values],
        textposition='outside',
        textfont=dict(color=CHART_THEME["text"]["primary"], size=13)
    ))

    fig.update_layout(
        **get_premium_layout(
            title="ë³´ì»¬ DNA",
            yaxis=dict(range=[0, 115], title="ì ìˆ˜")
        ),
        height=350,
        bargap=0.3
    )

    return fig


# =============================================
# ê¸°ìˆ ì  ë¶„ì„ ì‹œê°í™” í•¨ìˆ˜
# =============================================

def create_waveform_chart(y: np.ndarray, sr: int) -> go.Figure:
    """íŒŒí˜• ì°¨íŠ¸ ìƒì„±"""
    # ë‹¤ìš´ìƒ˜í”Œë§ (ì„±ëŠ¥ì„ ìœ„í•´)
    downsample_factor = max(1, len(y) // 5000)
    y_down = y[::downsample_factor]
    times = np.arange(len(y_down)) * downsample_factor / sr

    fig = go.Figure()
    fig.add_trace(go.Scatter(
        x=times, y=y_down,
        mode='lines',
        line=dict(color=CHART_THEME["colors"]["cyan"], width=0.8),
        fill='tozeroy',
        fillcolor='rgba(34, 211, 238, 0.1)',
        name='Waveform'
    ))

    fig.update_layout(
        **get_premium_layout(title="Waveform"),
        xaxis_title='Time (seconds)',
        yaxis_title='Amplitude',
        height=200,
        showlegend=False
    )

    return fig


def create_pitch_tracking_chart(f0: np.ndarray, times: np.ndarray, high_thresh: float, low_thresh: float) -> go.Figure:
    """í”¼ì¹˜ íŠ¸ë˜í‚¹ ì°¨íŠ¸ (ë ˆì§€ìŠ¤í„°ë³„ ìƒ‰ìƒ êµ¬ë¶„)"""
    fig = go.Figure()

    # ìœ íš¨í•œ í”¼ì¹˜ë§Œ ì¶”ì¶œ
    valid_mask = ~np.isnan(f0)
    valid_times = times[valid_mask]
    valid_f0 = f0[valid_mask]

    if len(valid_f0) > 0:
        # ì €ìŒ (íŒŒë‘)
        low_mask = valid_f0 < low_thresh
        if np.any(low_mask):
            fig.add_trace(go.Scatter(
                x=valid_times[low_mask], y=valid_f0[low_mask],
                mode='markers',
                marker=dict(color=CHART_THEME["colors"]["info"], size=4),
                name=f'Low (<{low_thresh:.0f}Hz)'
            ))

        # ì¤‘ìŒ (ì´ˆë¡)
        mid_mask = (valid_f0 >= low_thresh) & (valid_f0 <= high_thresh)
        if np.any(mid_mask):
            fig.add_trace(go.Scatter(
                x=valid_times[mid_mask], y=valid_f0[mid_mask],
                mode='markers',
                marker=dict(color=CHART_THEME["colors"]["success"], size=4),
                name='Mid'
            ))

        # ê³ ìŒ (ë¹¨ê°•)
        high_mask = valid_f0 > high_thresh
        if np.any(high_mask):
            fig.add_trace(go.Scatter(
                x=valid_times[high_mask], y=valid_f0[high_mask],
                mode='markers',
                marker=dict(color=CHART_THEME["colors"]["danger"], size=4),
                name=f'High (>{high_thresh:.0f}Hz)'
            ))

    fig.update_layout(
        **get_premium_layout(
            title="Pitch Tracking by Register",
            legend=dict(
                orientation='h',
                yanchor='bottom',
                y=1.02,
                xanchor='center',
                x=0.5
            )
        ),
        xaxis_title='Time (seconds)',
        yaxis_title='Frequency (Hz)',
        height=280
    )

    return fig


def create_dynamics_chart(rms_db: np.ndarray, times: np.ndarray) -> go.Figure:
    """ë‹¤ì´ë‚˜ë¯¹ìŠ¤ ì°¨íŠ¸"""
    fig = go.Figure()

    # ë°°ê²½ ì˜ì—­ (í˜¸í¡ ì„ê³„ê°’)
    breath_threshold = np.percentile(rms_db, 20)

    fig.add_trace(go.Scatter(
        x=times, y=rms_db,
        fill='tozeroy',
        mode='lines',
        line=dict(color=CHART_THEME["colors"]["gold"], width=1.5),
        fillcolor='rgba(201, 169, 98, 0.2)',
        name='Loudness'
    ))

    # í˜¸í¡ ì„ê³„ê°’ ë¼ì¸
    fig.add_hline(y=breath_threshold, line_dash="dash",
                  line_color=CHART_THEME["colors"]["warning"],
                  annotation_text="Breath Threshold",
                  annotation_font=dict(color=CHART_THEME["text"]["secondary"]))

    fig.update_layout(
        **get_premium_layout(title="Dynamics & Breath Pattern"),
        xaxis_title='Time (seconds)',
        yaxis_title='Loudness (dB)',
        height=220,
        showlegend=False
    )

    return fig


def create_pitch_distribution_chart(valid_f0: np.ndarray) -> go.Figure:
    """í”¼ì¹˜ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨"""
    fig = go.Figure()

    if len(valid_f0) > 0:
        fig.add_trace(go.Histogram(
            x=valid_f0,
            nbinsx=40,
            marker=dict(
                color=CHART_THEME["colors"]["purple"],
                line=dict(color=CHART_THEME["backgrounds"]["paper"], width=1)
            ),
            name='Pitch Distribution'
        ))

        # í‰ê·  í”¼ì¹˜ ë¼ì¸
        mean_pitch = np.mean(valid_f0)
        fig.add_vline(x=mean_pitch, line_dash="dash",
                      line_color=CHART_THEME["colors"]["gold"],
                      annotation_text=f"Mean: {mean_pitch:.0f}Hz",
                      annotation_font=dict(color=CHART_THEME["text"]["secondary"]))

    fig.update_layout(
        **get_premium_layout(title="Pitch Distribution"),
        xaxis_title='Frequency (Hz)',
        yaxis_title='Count',
        height=250,
        bargap=0.05,
        showlegend=False
    )

    return fig


def create_pitch_accuracy_chart(pitch_errors: np.ndarray) -> go.Figure:
    """í”¼ì¹˜ ì •í™•ë„ ë¶„í¬ (ì„¼íŠ¸ ë‹¨ìœ„)"""
    fig = go.Figure()

    if len(pitch_errors) > 0:
        # -50 ~ +50 cents ë²”ìœ„ë¡œ í´ë¦¬í•‘
        errors_clipped = np.clip(pitch_errors, -50, 50)

        fig.add_trace(go.Histogram(
            x=errors_clipped,
            nbinsx=40,
            marker=dict(
                color=CHART_THEME["colors"]["purple_light"],
                line=dict(color=CHART_THEME["backgrounds"]["paper"], width=1)
            ),
            name='Pitch Error'
        ))

        # ì™„ë²½í•œ í”¼ì¹˜ ë¼ì¸
        fig.add_vline(x=0, line_dash="solid",
                      line_color=CHART_THEME["colors"]["success"],
                      annotation_text="Perfect Pitch",
                      annotation_font=dict(color=CHART_THEME["colors"]["success"]))

        # í‰ê·  ì˜¤ì°¨
        mean_error = np.mean(pitch_errors)
        fig.add_vline(x=mean_error, line_dash="dash",
                      line_color=CHART_THEME["colors"]["warning"],
                      annotation_text=f"Mean: {mean_error:.1f}Â¢",
                      annotation_font=dict(color=CHART_THEME["text"]["secondary"]))

    fig.update_layout(
        **get_premium_layout(title="Pitch Accuracy Distribution"),
        xaxis_title='Pitch Error (cents)',
        yaxis_title='Count',
        height=250,
        bargap=0.05,
        showlegend=False
    )

    return fig


def create_spectral_centroid_chart(centroid: np.ndarray, times: np.ndarray) -> go.Figure:
    """ìŠ¤í™íŠ¸ëŸ´ ì„¼íŠ¸ë¡œì´ë“œ (ìŒìƒ‰ ë°ê¸°) ì°¨íŠ¸"""
    fig = go.Figure()

    fig.add_trace(go.Scatter(
        x=times, y=centroid,
        mode='lines',
        line=dict(color=CHART_THEME["colors"]["gold"], width=1.5),
        fill='tozeroy',
        fillcolor='rgba(201, 169, 98, 0.1)',
        name='Spectral Centroid'
    ))

    # Warm/Bright ê²½ê³„ì„  (MBTI ê¸°ì¤€ 1800Hz)
    boundary = 1800
    fig.add_hline(y=boundary, line_dash="dash",
                  line_color=CHART_THEME["text"]["muted"],
                  annotation_text="Warm/Bright (1800Hz)",
                  annotation_font=dict(color=CHART_THEME["text"]["secondary"]))

    fig.update_layout(
        **get_premium_layout(title="Tonal Brightness Over Time"),
        xaxis_title='Time (seconds)',
        yaxis_title='Spectral Centroid (Hz)',
        height=220,
        showlegend=False
    )

    return fig


def create_performance_summary_chart(features: dict, scorecard) -> go.Figure:
    """ì¢…í•© ì„±ëŠ¥ ìš”ì•½ ë°” ì°¨íŠ¸"""
    # ìŒìƒ‰ ë”°ëœ»í•¨ì€ ë³„ë„ í•´ì„ í•„ìš” (40%+ = ë”°ëœ»í•¨, 27-40% = ê· í˜•, <27% = ë°ìŒ)
    warmth_raw = features['warmth_score'] * 100
    if warmth_raw >= 40:
        warmth_label = "ë”°ëœ»í•¨"
    elif warmth_raw >= 27:
        warmth_label = "ê· í˜•"
    else:
        warmth_label = "ë°ìŒ"

    categories = ['Pitch\nAccuracy', 'High Note\nControl', 'Breath\nSupport', 'Dynamics', f'Tone\n({warmth_label})']

    # ì ìˆ˜ ê³„ì‚° (0-100 ìŠ¤ì¼€ì¼)
    pitch_score = max(0, min(100, 100 - features['pitch_accuracy_cents'] * 2))
    high_note_score = features['high_note_stability'] * 100
    breath_score = min(100, features['breath_phrase_length'] * 15)
    dynamics_score = min(100, features['dynamic_range_db'] * 4)
    warmth_score = warmth_raw

    values = [pitch_score, high_note_score, breath_score, dynamics_score, warmth_score]
    colors = [
        CHART_THEME["colors"]["info"],
        CHART_THEME["colors"]["purple"],
        CHART_THEME["colors"]["success"],
        CHART_THEME["colors"]["warning"],
        CHART_THEME["colors"]["pink"]
    ]

    fig = go.Figure()

    fig.add_trace(go.Bar(
        x=categories,
        y=values,
        marker=dict(color=colors, line=dict(width=0)),
        text=[f'{v:.0f}' for v in values],
        textposition='outside',
        textfont=dict(color=CHART_THEME["text"]["primary"], size=13)
    ))

    # ê¸°ì¤€ì„ 
    fig.add_hline(y=70, line_dash="dash", line_color=CHART_THEME["colors"]["success"],
                  annotation_text="Good (70)",
                  annotation_font=dict(color=CHART_THEME["text"]["secondary"]))
    fig.add_hline(y=40, line_dash="dot", line_color=CHART_THEME["colors"]["pink"],
                  annotation_text="Warm Tone (40+)",
                  annotation_font=dict(color=CHART_THEME["text"]["secondary"]))

    fig.update_layout(
        **get_premium_layout(
            title="Vocal Performance Summary",
            yaxis=dict(range=[0, 115], title='Score')
        ),
        height=320,
        bargap=0.3
    )

    return fig


def hz_to_note_name(hz: float) -> str:
    """ì£¼íŒŒìˆ˜ë¥¼ ìŒì´ë¦„ìœ¼ë¡œ ë³€í™˜"""
    if hz <= 0:
        return "N/A"
    midi = librosa.hz_to_midi(hz)
    note_names = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']
    note_idx = int(round(midi)) % 12
    octave = int(round(midi)) // 12 - 1
    return f"{note_names[note_idx]}{octave}"


def create_comparison_bar_chart(features_a: dict, features_b: dict, title_a: str, title_b: str) -> go.Figure:
    """ë‘ ê³¡ì˜ íŠ¹ì§• ë¹„êµ ë°” ì°¨íŠ¸"""
    categories = ['ìŒì—­í­\n(ë°˜ìŒ)', 'ë‹¤ì´ë‚˜ë¯¹\n(dB)', 'ê³ ìŒ ì•ˆì •ì„±\n(%)', 'ìŒìƒ‰ ë°ê¸°\n(ì ìˆ˜)', 'ìŒì • ì •í™•ë„\n(ì ìˆ˜)']

    # ì •ê·œí™”ëœ ê°’ìœ¼ë¡œ ë³€í™˜ (0-100 ìŠ¤ì¼€ì¼)
    values_a = [
        min(100, features_a['pitch_range_semitones'] * 4),  # 25ë°˜ìŒ = 100
        min(100, features_a['dynamic_range_db'] * 4),  # 25dB = 100
        features_a['high_note_stability'] * 100,
        (1 - features_a['warmth_score']) * 100,  # ë°ê¸° (warmth ë°˜ì „)
        max(0, 100 - features_a['pitch_accuracy_cents'] * 2)  # ì •í™•ë„
    ]

    values_b = [
        min(100, features_b['pitch_range_semitones'] * 4),
        min(100, features_b['dynamic_range_db'] * 4),
        features_b['high_note_stability'] * 100,
        (1 - features_b['warmth_score']) * 100,
        max(0, 100 - features_b['pitch_accuracy_cents'] * 2)
    ]

    fig = go.Figure()

    fig.add_trace(go.Bar(
        name=title_a,
        x=categories,
        y=values_a,
        marker=dict(color=CHART_THEME["colors"]["gold"], line=dict(width=0)),
        text=[f'{v:.0f}' for v in values_a],
        textposition='outside',
        textfont=dict(color=CHART_THEME["text"]["primary"], size=12)
    ))

    fig.add_trace(go.Bar(
        name=title_b,
        x=categories,
        y=values_b,
        marker=dict(color=CHART_THEME["colors"]["purple"], line=dict(width=0)),
        text=[f'{v:.0f}' for v in values_b],
        textposition='outside',
        textfont=dict(color=CHART_THEME["text"]["primary"], size=12)
    ))

    fig.update_layout(
        **get_premium_layout(
            title="Song A vs Song B ë¹„êµ",
            legend=dict(
                orientation='h',
                yanchor='bottom',
                y=1.02,
                xanchor='center',
                x=0.5
            ),
            yaxis=dict(range=[0, 115], title='Score')
        ),
        barmode='group',
        height=380,
        bargap=0.2
    )

    return fig


def create_evidence_chart(evidence: dict, title: str) -> go.Figure:
    """ê·¼ê±° ë°ì´í„°ë¥¼ ì‹œê°ì  ì°¨íŠ¸ë¡œ ë³€í™˜"""
    if not evidence:
        return None

    # í‚¤-ê°’ ì •ë¦¬
    labels = []
    values = []
    colors = []

    for key, value in evidence.items():
        # í‚¤ ì´ë¦„ ì •ë¦¬
        display_key = key.replace('_', ' ').replace('slow', 'Song A').replace('fast', 'Song B')
        labels.append(display_key.title())

        # ê°’ì„ 0-100 ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜
        if isinstance(value, (int, float)):
            if value <= 1:
                values.append(value * 100)
            else:
                values.append(min(100, value))
        else:
            values.append(50)  # ê¸°ë³¸ê°’

        # ìƒ‰ìƒ (ê°’ì— ë”°ë¼)
        if values[-1] >= 70:
            colors.append(CHART_THEME["colors"]["success"])  # ì¢‹ìŒ
        elif values[-1] >= 40:
            colors.append(CHART_THEME["colors"]["warning"])  # ë³´í†µ
        else:
            colors.append(CHART_THEME["colors"]["danger"])  # ê°œì„ í•„ìš”

    fig = go.Figure()

    fig.add_trace(go.Bar(
        x=labels,
        y=values,
        marker=dict(color=colors, line=dict(width=0)),
        text=[f'{v:.0f}' for v in values],
        textposition='outside',
        textfont=dict(color=CHART_THEME["text"]["primary"], size=12)
    ))

    fig.update_layout(
        **get_premium_layout(
            title=title,
            yaxis=dict(range=[0, 115], title='Score')
        ),
        height=280,
        bargap=0.3
    )

    return fig


def render_technical_analysis(features: dict, scorecard=None, key_prefix: str = "main"):
    """ê¸°ìˆ ì  ë¶„ì„ íƒ­ ë Œë”ë§ - ì‹¬ì¸µ ë³´ì»¬ ë¶„ì„ ë¦¬í¬íŠ¸"""
    ts = features.get('timeseries', {})

    if not ts:
        st.warning("ì‹œê³„ì—´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¶„ì„ì„ ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return

    # =========================================
    # ğŸ“Š ë³´ì»¬ ê¸°ìˆ  ë¶„ì„ ê²°ê³¼ í—¤ë”
    # =========================================
    st.markdown("## ğŸ“Š ë³´ì»¬ ê¸°ìˆ  ë¶„ì„ ê²°ê³¼")

    # =========================================
    # ğŸ¤ 1. ìŒì—­ëŒ€ (Vocal Range)
    # =========================================
    st.subheader("ğŸ¤ ìŒì—­ëŒ€ (Vocal Range)")

    min_hz = features['pitch_min_hz']
    max_hz = features['pitch_max_hz']
    avg_hz = features['avg_pitch_hz']
    range_semitones = features['pitch_range_semitones']
    octaves = range_semitones / 12

    # ìŒì—­ëŒ€ í•´ì„
    if avg_hz < 165:  # A2 ì´í•˜
        avg_interpret = "ì €ìŒì—­ (ë² ì´ìŠ¤~ë°”ë¦¬í†¤)"
    elif avg_hz < 262:  # C4 ì´í•˜
        avg_interpret = "ì¤‘ì €ìŒì—­ (ë°”ë¦¬í†¤~í…Œë„ˆ)"
    elif avg_hz < 392:  # G4 ì´í•˜
        avg_interpret = "ì¤‘ê³ ìŒì—­ (í…Œë„ˆ~ì•Œí† )"
    else:
        avg_interpret = "ê³ ìŒì—­ (ì†Œí”„ë¼ë…¸)"

    if octaves < 1.5:
        range_interpret = "ì¢ì€ ìŒì—­ (íŠ¹ì • ê³¡ì— íŠ¹í™”)"
    elif octaves < 2.0:
        range_interpret = "ì¼ë°˜ì ì¸ ëŒ€ì¤‘ìŒì•… ê°€ì°½ ë²”ìœ„"
    elif octaves < 2.5:
        range_interpret = "ë„“ì€ ìŒì—­ (ë‹¤ì–‘í•œ ê³¡ ì†Œí™” ê°€ëŠ¥)"
    else:
        range_interpret = "ë§¤ìš° ë„“ì€ ìŒì—­ (ì „ë¬¸ ê°€ìˆ˜ê¸‰)"

    col1, col2 = st.columns(2)
    with col1:
        st.markdown("""
        | í•­ëª© | ê°’ | í•´ì„ |
        |------|-----|------|
        | ìµœì €ìŒ | {} ({:.0f}Hz) | ì €ìŒì—­ |
        | ìµœê³ ìŒ | {} ({:.0f}Hz) | ê³ ìŒì—­ |
        | í‰ê· ìŒ | {} ({:.0f}Hz) | {} |
        | ìŒì—­í­ | {:.1f} ì˜¥íƒ€ë¸Œ | {} |
        """.format(
            hz_to_note_name(min_hz), min_hz,
            hz_to_note_name(max_hz), max_hz,
            hz_to_note_name(avg_hz), avg_hz, avg_interpret,
            octaves, range_interpret
        ))

    with col2:
        # ì°¨íŠ¸: í”¼ì¹˜ ë¶„í¬
        pitch_dist_fig = create_pitch_distribution_chart(ts['valid_f0'])
        st.plotly_chart(pitch_dist_fig, use_container_width=True, key=f"{key_prefix}_pitch_dist_range")

    st.info(f"**í•´ì„**: ìŒì—­ëŒ€ëŠ” {octaves:.1f}ì˜¥íƒ€ë¸Œë¡œ {range_interpret}. í‰ê· ìŒ {hz_to_note_name(avg_hz)}ì€ {avg_interpret}ì— í•´ë‹¹í•©ë‹ˆë‹¤.")

    st.markdown("---")

    # =========================================
    # ğŸ”Š 2. ë‹¤ì´ë‚˜ë¯¹ìŠ¤ (ê°•ì•½ í‘œí˜„)
    # =========================================
    st.subheader("ğŸ”Š ë‹¤ì´ë‚˜ë¯¹ìŠ¤ (ê°•ì•½ í‘œí˜„)")

    dynamic_range = features['dynamic_range_db']
    rms_mean = features['rms_db_mean']
    rms_max = features['rms_db_max']
    climax_intensity = features['climax_intensity']

    # ë‹¤ì´ë‚˜ë¯¹ìŠ¤ í•´ì„
    if dynamic_range < 10:
        dyn_interpret = "ì¢ìŒ (ë‹¨ì¡°ë¡œìš´ í‘œí˜„)"
        dyn_tip = "ë” ê·¹ì ì¸ ê°•ì•½ ëŒ€ë¹„ë¥¼ ì—°ìŠµí•´ë³´ì„¸ìš”"
    elif dynamic_range < 15:
        dyn_interpret = "ë³´í†µ (ì ì ˆí•œ í‘œí˜„)"
        dyn_tip = "ì¢€ ë” ë‹¤ì´ë‚˜ë¯¹í•œ í‘œí˜„ì„ ì¶”ê°€í•˜ë©´ ì°¬ì–‘ì´ í’ì„±í•´ì§‘ë‹ˆë‹¤"
    elif dynamic_range < 20:
        dyn_interpret = "ë„“ìŒ (í’ë¶€í•œ í‘œí˜„)"
        dyn_tip = "ë‹¤ì´ë‚˜ë¯¹ í‘œí˜„ì´ ì˜ ë˜ì–´ ìˆìŠµë‹ˆë‹¤"
    else:
        dyn_interpret = "ë§¤ìš° ë„“ìŒ (ì „ë¬¸ì  í‘œí˜„ë ¥)"
        dyn_tip = "í›Œë¥­í•œ ë‹¤ì´ë‚˜ë¯¹ ì»¨íŠ¸ë¡¤ì…ë‹ˆë‹¤"

    col1, col2 = st.columns(2)
    with col1:
        st.markdown(f"""
        | í•­ëª© | ê°’ | í•´ì„ |
        |------|-----|------|
        | ë‹¤ì´ë‚˜ë¯¹ ë ˆì¸ì§€ | {dynamic_range:.1f}dB | {dyn_interpret} |
        | í‰ê·  ìŒëŸ‰ | {rms_mean:.1f}dB | - |
        | ìµœëŒ€ ìŒëŸ‰ | {rms_max:.1f}dB | - |
        | í´ë¼ì´ë§¥ìŠ¤ ê°•ë„ | {climax_intensity:.2f}x | {"ê°•í•œ ì ˆì •" if climax_intensity > 1.5 else "ì ì ˆí•œ ì ˆì •"} |
        """)

        with st.expander("ğŸ“– ë‹¤ì´ë‚˜ë¯¹ìŠ¤ ì´ë¡ "):
            st.markdown("""
            **ë‹¤ì´ë‚˜ë¯¹ ë ˆì¸ì§€ ê¸°ì¤€:**
            - 10dB ì´í•˜: ë‹¨ì¡°ë¡œìš´ í‘œí˜„
            - 10-15dB: ë³´í†µ ìˆ˜ì¤€
            - 15-20dB: í’ë¶€í•œ í‘œí˜„
            - 20dB ì´ìƒ: ì „ë¬¸ê°€ ìˆ˜ì¤€

            **ê°œì„  íŒ**: pp(ë§¤ìš° ì—¬ë¦¬ê²Œ) ~ ff(ë§¤ìš° ê°•í•˜ê²Œ) í­ì„ ë„“íˆë©´ ê·¹ì ì¸ ì°¬ì–‘ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
            """)

    with col2:
        # íŒŒí˜• ì°¨íŠ¸
        waveform_fig = create_waveform_chart(ts['waveform'], features['sample_rate'])
        st.plotly_chart(waveform_fig, use_container_width=True, key=f"{key_prefix}_waveform_dynamics")

    # ë‹¤ì´ë‚˜ë¯¹ìŠ¤ ì°¨íŠ¸
    dynamics_fig = create_dynamics_chart(ts['rms_db'], ts['rms_times'])
    st.plotly_chart(dynamics_fig, use_container_width=True, key=f"{key_prefix}_dynamics_main")

    st.info(f"**í•´ì„**: {dyn_interpret}. {dyn_tip}")

    st.markdown("---")

    # =========================================
    # ğŸ¨ 3. ìŒìƒ‰ (Timbre)
    # =========================================
    st.subheader("ğŸ¨ ìŒìƒ‰ (Timbre)")

    avg_centroid = features['spectral_centroid_hz']
    warmth_pct = features['warmth_score'] * 100

    if avg_centroid < 1800:
        tone_type = "ë”°ëœ»í•˜ê³  ë¶€ë“œëŸ¬ìš´"
        tone_suit = "ë°œë¼ë“œ, ì°¬ì–‘ì— ì í•©"
        tone_tip = "í•„ìš”ì‹œ ê³ ìŒì—­ì—ì„œ ì¢€ ë” ë°ì€ ë°œì„±ì„ ì„ìœ¼ë©´ í™˜í•˜ê²Œ í¼ì§€ëŠ” ëŠë‚Œì„ ì¤„ ìˆ˜ ìˆì–´ìš”"
    elif avg_centroid < 2200:
        tone_type = "ê· í˜• ì¡íŒ"
        tone_suit = "ë‹¤ì–‘í•œ ì¥ë¥´ì— ì í•©"
        tone_tip = "ê· í˜• ì¡íŒ ìŒìƒ‰ìœ¼ë¡œ ë‹¤ì–‘í•œ ê³¡ì„ ì†Œí™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"
    else:
        tone_type = "ë°ê³  ì„ ëª…í•œ"
        tone_suit = "ì—…í…œí¬, CCMì— ì í•©"
        tone_tip = "ì¡°ìš©í•œ ê³¡ì—ì„œëŠ” ì˜ë„ì ìœ¼ë¡œ ë¶€ë“œëŸ¬ìš´ ë°œì„±ì„ ì‚¬ìš©í•´ë³´ì„¸ìš”"

    col1, col2 = st.columns(2)
    with col1:
        st.markdown(f"""
        | í•­ëª© | ê°’ | í•´ì„ |
        |------|-----|------|
        | ìŒìƒ‰ ë°ê¸° | {avg_centroid:.0f}Hz | {tone_type} ìŒìƒ‰ |
        | ìŒìƒ‰ ë”°ëœ»í•¨ | {warmth_pct:.0f}% | {tone_suit} |
        """)
        st.caption("ğŸ’¡ ìŒìƒ‰ ë°ê¸°: ë‚®ì„ìˆ˜ë¡ ë”°ëœ»í•˜ê³ , ë†’ì„ìˆ˜ë¡ ë°ì€ ìŒìƒ‰ì…ë‹ˆë‹¤.")

    with col2:
        # ìŒìƒ‰ ë°ê¸° ì°¨íŠ¸
        centroid_fig = create_spectral_centroid_chart(ts['centroid'], ts['centroid_times'])
        st.plotly_chart(centroid_fig, use_container_width=True, key=f"{key_prefix}_centroid_timbre")

    st.info(f"**í•´ì„**: {tone_type} ìŒìƒ‰ìœ¼ë¡œ {tone_suit}ì…ë‹ˆë‹¤. {tone_tip}")

    st.markdown("---")

    # =========================================
    # ğŸ“ˆ 4. ì•ˆì •ì„± (Pitch Stability)
    # =========================================
    st.subheader("ğŸ“ˆ ì•ˆì •ì„± (Pitch Stability)")

    pitch_std = features['pitch_std']
    pitch_stability = features.get('pitch_stability', 0.5)
    vibrato_ratio = features.get('vibrato_ratio', 0.3)

    # ì•ˆì •ì„± í•´ì„
    stability_pct = pitch_stability * 100
    vibrato_pct = vibrato_ratio * 100

    if stability_pct >= 70:
        stability_interpret = "ì•ˆì •ì  (ì¢‹ìŒ)"
    elif stability_pct >= 50:
        stability_interpret = "ë³´í†µ"
    else:
        stability_interpret = "ë³€ë™ì´ í° í¸"

    col1, col2 = st.columns(2)
    with col1:
        st.markdown(f"""
        | í•­ëª© | ê°’ | í•´ì„ |
        |------|-----|------|
        | í”¼ì¹˜ ë³€ë™ì„± | {pitch_std:.1f}Hz | {"ë‚®ìŒ (ì•ˆì •ì )" if pitch_std < 30 else "ë³€ë™ ìˆìŒ"} |
        | í”¼ì¹˜ ì•ˆì •ì„± | {stability_pct:.0f}% | {stability_interpret} |
        | ë¹„ë¸Œë¼í†  ë¹„ìœ¨ | {vibrato_pct:.0f}% | {"í™œë°œí•œ ê°ì • í‘œí˜„" if vibrato_pct > 30 else "ì ˆì œëœ í‘œí˜„"} |
        """)

        with st.expander("ğŸ“– ì•ˆì •ì„± í•´ì„"):
            st.markdown("""
            í”¼ì¹˜ ë³€ë™ì´ í° ê²½ìš° ë‘ ê°€ì§€ ê°€ëŠ¥ì„±ì´ ìˆì–´ìš”:

            1. **í‘œí˜„ë ¥** - ì˜ë„ì ì¸ ë¹„ë¸Œë¼í† , ê¾¸ë°ˆìŒ, ê°ì • í‘œí˜„
            2. **ìŒì • ë¶ˆì•ˆì •** - ë¹„ì˜ë„ì ì¸ í”¼ì¹˜ í”ë“¤ë¦¼

            ê·¸ë˜í”„ë¥¼ ë³´ê³  ë¡±í†¤(ê¸´ ìŒ)ì—ì„œ í”¼ì¹˜ê°€ í”ë“¤ë¦¬ëŠ”ì§€ í™•ì¸í•´ë³´ì„¸ìš”.
            """)

    with col2:
        # í”¼ì¹˜ íŠ¸ë˜í‚¹ ì°¨íŠ¸
        pitch_tracking_fig = create_pitch_tracking_chart(
            ts['f0'], ts['f0_times'],
            features['high_threshold_hz'],
            features['low_threshold_hz']
        )
        st.plotly_chart(pitch_tracking_fig, use_container_width=True, key=f"{key_prefix}_pitch_tracking_stability")

    st.markdown("---")

    # =========================================
    # ğŸµ 5. ê³ ìŒ ì²˜ë¦¬ (High Note Technique)
    # =========================================
    st.subheader("ğŸµ ê³ ìŒ ì²˜ë¦¬ (High Note Technique)")

    high_note_ratio = features.get('high_note_ratio', 0.15)
    high_note_stability = features.get('high_note_stability', 0.8)

    high_ratio_pct = high_note_ratio * 100
    high_stability_pct = high_note_stability * 100

    # ê³ ìŒ ì•ˆì •ì„± í•´ì„
    if high_stability_pct >= 85:
        high_stability_interpret = "âœ… ì•ˆì •ì "
    elif high_stability_pct >= 70:
        high_stability_interpret = "ì–‘í˜¸"
    else:
        high_stability_interpret = "âš ï¸ í”ë“¤ë¦¼ ìˆìŒ"

    col1, col2 = st.columns(2)
    with col1:
        st.markdown(f"""
        | í•­ëª© | ë¶„ì„ ê²°ê³¼ | ë³´ì»¬ ì´ë¡ ì  í•´ì„ |
        |------|----------|-----------------|
        | ìµœê³ ìŒ | {hz_to_note_name(max_hz)} ({max_hz:.0f}Hz) | {"ë‚¨ì„± í…Œë„ˆ ê¸°ì¤€ ì ì ˆ" if max_hz < 500 else "ì—¬ì„±/í…Œë„ˆ ê³ ìŒì—­"} |
        | ê³ ìŒì—­ ë¹„ìœ¨ | {high_ratio_pct:.1f}% | {"ê³ ìŒì„ ì ì ˆíˆ í™œìš©" if high_ratio_pct > 10 else "ê³ ìŒ ì‚¬ìš© ì ìŒ"} |
        | ê³ ìŒ ì•ˆì •ì„± | {high_stability_pct:.0f}% | {high_stability_interpret} |
        """)

        with st.expander("ğŸ“– ë³´ì»¬ ì´ë¡  í•´ì„"):
            st.markdown("""
            **ê³ ìŒ ì•ˆì •ì„±**ì´ ë‚®ì€ ê²½ìš°:
            - **ë‘ì„±(Head Voice)ê³¼ í‰ì„±(Chest Voice)ì˜ ì „í™˜ì (Passaggio)**ì—ì„œ ë°œìƒí•  ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤
            - **ë¯¹ìŠ¤ë³´ì´ìŠ¤(Mixed Voice)** í›ˆë ¨ìœ¼ë¡œ ê°œì„  ê°€ëŠ¥

            **ğŸ’¡ ê°œì„  íŒ**: ë¦½íŠ¸ë¦´(Lip Trill)ì´ë‚˜ í—˜ë°ìœ¼ë¡œ ê³ ìŒ êµ¬ê°„ì„ ì—°ìŠµí•˜ë©´
            ì„±ëŒ€ ê¸´ì¥ ì—†ì´ ê³ ìŒì„ ì•ˆì •ì ìœ¼ë¡œ ìœ ì§€í•  ìˆ˜ ìˆì–´ìš”.
            """)

    with col2:
        # ì„±ëŠ¥ ìš”ì•½ ì°¨íŠ¸
        performance_fig = create_performance_summary_chart(features, scorecard)
        st.plotly_chart(performance_fig, use_container_width=True, key=f"{key_prefix}_performance_highnote")

    st.markdown("---")

    # =========================================
    # ğŸŒ¬ï¸ 6. í˜¸í¡ ë¶„ì„ (Breath & Appoggio)
    # =========================================
    st.subheader("ğŸŒ¬ï¸ í˜¸í¡ ë¶„ì„ (Breath & Appoggio)")

    breath_phrase = features.get('breath_phrase_length', 3.0)
    voiced_ratio = features.get('voiced_ratio', 0.7)

    # í˜¸í¡ í•´ì„
    if breath_phrase < 3.0:
        breath_interpret = "âš ï¸ ì§§ì€ í¸"
        breath_tip = "ë³µì‹í˜¸í¡ì„ í†µí•œ íš¡ê²©ë§‰ ì»¨íŠ¸ë¡¤ì´ ë” í•„ìš”í•´ìš”"
    elif breath_phrase < 5.0:
        breath_interpret = "ë³´í†µ"
        breath_tip = "í˜¸í¡ ì§€ì§€ê°€ ì–´ëŠ ì •ë„ ë˜ê³  ìˆìŠµë‹ˆë‹¤"
    else:
        breath_interpret = "âœ… ìš°ìˆ˜"
        breath_tip = "í˜¸í¡ ì»¨íŠ¸ë¡¤ì´ ì˜ ë˜ì–´ ìˆìŠµë‹ˆë‹¤"

    col1, col2 = st.columns(2)
    with col1:
        st.markdown(f"""
        | í•­ëª© | ë¶„ì„ ê²°ê³¼ | í•´ì„ |
        |------|----------|------|
        | í‰ê·  í”„ë ˆì´ì¦ˆ | {breath_phrase:.1f}ì´ˆ | {breath_interpret} |
        | ë°œì„± ë¹„ìœ¨ | {voiced_ratio*100:.0f}% | {"ì•ˆì •ì  ë°œì„±" if voiced_ratio > 0.6 else "ëŠê¹€ ìˆìŒ"} |
        """)

        with st.expander("ğŸ“– ë³´ì»¬ ì´ë¡  í•´ì„"):
            st.markdown(f"""
            **ì´íƒˆë¦¬ì•„ ë²¨ì¹¸í†  ì´ë¡ ì˜ Appoggio(í˜¸í¡ ì§€ì§€)** ê´€ì ì—ì„œ:

            - í”„ë ˆì´ì¦ˆ {breath_phrase:.1f}ì´ˆëŠ” {breath_interpret}ì…ë‹ˆë‹¤
            - {breath_tip}

            **ğŸ’¡ ê°œì„  íŒ**: "Sustained Hissing Exercise" - 's' ì†Œë¦¬ë¥¼ ì¼ì •í•˜ê²Œ 30ì´ˆ ì´ìƒ ìœ ì§€í•˜ëŠ” ì—°ìŠµì„ í†µí•´
            íš¡ê²©ë§‰ ì¡°ì ˆë ¥ì„ í‚¤ìš°ë©´ í”„ë ˆì´ì¦ˆ ê¸¸ì´ê°€ ëŠ˜ì–´ë‚©ë‹ˆë‹¤.
            """)

    with col2:
        # ë‹¤ì´ë‚˜ë¯¹ìŠ¤ ì°¨íŠ¸ (í˜¸í¡ íŒ¨í„´ í™•ì¸ìš©)
        dynamics_fig2 = create_dynamics_chart(ts['rms_db'], ts['rms_times'])
        st.plotly_chart(dynamics_fig2, use_container_width=True, key=f"{key_prefix}_dynamics_breath")

    st.markdown("---")

    # =========================================
    # ğŸ¯ 7. ìŒì • ë¶„ì„ (Intonation)
    # =========================================
    st.subheader("ğŸ¯ ìŒì • ë¶„ì„ (Intonation)")

    accuracy_cents = features['pitch_accuracy_cents']
    flat_pct = features['flat_tendency'] * 100
    sharp_pct = features['sharp_tendency'] * 100
    vibrato_pct = features.get('vibrato_ratio', 0.3) * 100

    # ìŒì • ë“±ê¸‰
    if accuracy_cents < 10:
        accuracy_grade = "A+ (í”„ë¡œ ìˆ˜ì¤€)"
        accuracy_desc = "ê±°ì˜ ì¸ì§€ ë¶ˆê°€"
    elif accuracy_cents < 15:
        accuracy_grade = "A (ë§¤ìš° ì •í™•)"
        accuracy_desc = "ë¯¸ì„¸í•˜ê²Œ ì¸ì§€"
    elif accuracy_cents < 25:
        accuracy_grade = "B (ì–‘í˜¸)"
        accuracy_desc = "ë¯¸ì„¸í•˜ê²Œ ì¸ì§€ ê°€ëŠ¥"
    elif accuracy_cents < 50:
        accuracy_grade = "C (ë³´í†µ)"
        accuracy_desc = "ì²­ì¤‘ì´ ì¸ì§€ ê°€ëŠ¥"
    else:
        accuracy_grade = "D (ê°œì„  í•„ìš”)"
        accuracy_desc = "ëª…í™•í•œ ìŒì´íƒˆ"

    # ê²½í–¥ ë¶„ì„
    if flat_pct > sharp_pct + 10:
        tendency = f"âš ï¸ í”Œë« ê²½í–¥ ({flat_pct:.0f}%)"
        tendency_reason = "í˜¸í¡ ì§€ì§€ ì•½í™” ë˜ëŠ” í›„ë‘ ìœ„ì¹˜ê°€ ë‚®ê²Œ ìœ ì§€ë˜ì–´ ë°œìƒ"
    elif sharp_pct > flat_pct + 10:
        tendency = f"ìƒ¤í”„ ê²½í–¥ ({sharp_pct:.0f}%)"
        tendency_reason = "ê¸´ì¥ ë˜ëŠ” í˜¸í¡ ì••ë ¥ì´ ë†’ì•„ ë°œìƒ"
    else:
        tendency = "ê· í˜•ì¡íŒ ìŒì •"
        tendency_reason = "ì•ˆì •ì ì¸ ìŒì • ì»¨íŠ¸ë¡¤"

    # P0: ë°±ë¶„ìœ„ ë±ƒì§€ ê³„ì‚°
    pitch_percentile, pitch_delta_type = get_percentile_badge("pitch_accuracy_cents", accuracy_cents)
    high_note_percentile, _ = get_percentile_badge("high_note_stability", high_stability_pct)

    col1, col2 = st.columns(2)
    with col1:
        st.markdown(f"""
        | í•­ëª© | ê²°ê³¼ | í•´ì„ |
        |------|------|------|
        | í‰ê·  ì˜¤ì°¨ | {accuracy_cents:.1f} cents | {accuracy_grade} |
        | ìƒ¤í”„ ê²½í–¥ | {sharp_pct:.0f}% | - |
        | í”Œë« ê²½í–¥ | {flat_pct:.0f}% | {tendency} |
        | ë¹„ë¸Œë¼í†  | {vibrato_pct:.0f}% | {"âœ… í™œë°œí•œ ê°ì • í‘œí˜„" if vibrato_pct > 30 else "ì ˆì œëœ í‘œí˜„"} |
        """)

        # P0: ë°±ë¶„ìœ„ í‘œì‹œ
        if pitch_percentile:
            badge_color = "green" if "ìƒìœ„" in pitch_percentile else ("orange" if "í‰ê· " in pitch_percentile else "red")
            st.markdown(f"ğŸ… **ìŒì • ì •í™•ë„**: `{pitch_percentile}`")

        with st.expander("ğŸ“– ìŒì • ì˜¤ì°¨ ê¸°ì¤€ (Professional Standard)"):
            st.markdown("""
            | ë²”ìœ„ | ë“±ê¸‰ | ì„¤ëª… |
            |------|------|------|
            | 0-10 cents | í”„ë¡œ ìˆ˜ì¤€ | ê±°ì˜ ì¸ì§€ ë¶ˆê°€ |
            | 10-25 cents | ì–‘í˜¸ | ë¯¸ì„¸í•˜ê²Œ ì¸ì§€ |
            | 25-50 cents | ë³´í†µ | ì²­ì¤‘ì´ ì¸ì§€ ê°€ëŠ¥ |
            | 50+ cents | ê°œì„  í•„ìš” | ëª…í™•í•œ ìŒì´íƒˆ |

            **Bel Canto ì „í†µ**ì—ì„œ ê¶Œì¥í•˜ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ ë¹„ë¸Œë¼í† ëŠ” 5-7Hz, 30-50 cents í­ì…ë‹ˆë‹¤.
            """)

    with col2:
        # í”¼ì¹˜ ì •í™•ë„ íˆìŠ¤í† ê·¸ë¨
        pitch_accuracy_fig = create_pitch_accuracy_chart(ts['pitch_errors'])
        st.plotly_chart(pitch_accuracy_fig, use_container_width=True, key=f"{key_prefix}_pitch_accuracy_intonation")

    st.info(f"**ìŒì • ë¶„ì„**: í‰ê·  ì˜¤ì°¨ {accuracy_cents:.1f}cents ({accuracy_grade}) | {tendency}\n\n*{tendency_reason}*")

    if flat_pct > sharp_pct + 10:
        st.warning("ğŸ’¡ **í”Œë« ê²½í–¥ êµì • íŒ**: 'ë†’ê²Œ ìƒê°í•˜ê³  ë…¸ë˜í•˜ê¸°' - íƒ€ê²Ÿ ìŒë³´ë‹¤ ì•½ê°„ ìœ„ë¥¼ ì¡°ì¤€í•˜ëŠ” ì˜ì‹ì  ì—°ìŠµ. í”¼ì•„ë…¸ì™€ í•¨ê»˜ ìŠ¤ì¼€ì¼ ì—°ìŠµ ì‹œ ìŒì •ì„ ë…¹ìŒí•´ì„œ í”¼ë“œë°± ë°›ê¸°.")

    st.markdown("---")

    # =========================================
    # ğŸ“Š 8. ì¢…í•© ì ìˆ˜ & í”¼ë“œë°±
    # =========================================
    st.subheader("ğŸ“Š ì¢…í•© ì ìˆ˜ & í”¼ë“œë°±")

    # ì ìˆ˜ ê³„ì‚° (ë†’ì„ìˆ˜ë¡ ì¢‹ì€ í•­ëª©ë§Œ)
    pitch_score = max(0, min(100, 100 - accuracy_cents * 2))
    high_note_score = high_stability_pct
    breath_score = min(100, breath_phrase * 15)
    dynamics_score = min(100, dynamic_range * 5)

    scores = {
        "ìŒì • ì •í™•ë„": pitch_score,
        "ê³ ìŒ ì»¨íŠ¸ë¡¤": high_note_score,
        "í˜¸í¡ ì§€ì§€": breath_score,
        "ë‹¤ì´ë‚˜ë¯¹": dynamics_score
    }

    col1, col2 = st.columns(2)
    with col1:
        st.markdown("**ì˜ì—­ë³„ ì ìˆ˜**")
        for label, score in scores.items():
            if score >= 80:
                emoji = "ğŸŸ¢"
            elif score >= 60:
                emoji = "ğŸŸ¡"
            else:
                emoji = "ğŸ”´"
            st.markdown(f"{emoji} **{label}**: {score:.0f}/100")

        # ìŒìƒ‰ íŠ¹ì„± (ì ìˆ˜ê°€ ì•„ë‹Œ ìŠ¤í™íŠ¸ëŸ¼ìœ¼ë¡œ í‘œì‹œ)
        st.markdown("---")
        st.markdown("**ğŸ¨ ìŒìƒ‰ íŠ¹ì„±** *(ë†’ê³  ë‚®ìŒì´ ì•„ë‹Œ íŠ¹ì„±)*")
        if warmth_pct >= 50:
            tone_char = "ğŸ”¥ ë”°ëœ»í•œ ìŒìƒ‰"
            tone_desc = "ë°œë¼ë“œ, ì°¬ì–‘ ì¸ë„ì— ì í•©"
        elif warmth_pct >= 30:
            tone_char = "âš–ï¸ ê· í˜• ì¡íŒ ìŒìƒ‰"
            tone_desc = "ë‹¤ì–‘í•œ ì¥ë¥´ì— ì í•©"
        else:
            tone_char = "âœ¨ ë°ì€ ìŒìƒ‰"
            tone_desc = "ì—…í…œí¬, CCMì— ì í•©"

        # ìŠ¤í™íŠ¸ëŸ¼ ë°” ì‹œê°í™”
        st.markdown(f"**{tone_char}** - {tone_desc}")
        st.markdown(f"```\në”°ëœ»í•¨ {'â–ˆ' * int(warmth_pct / 10)}{'â–‘' * (10 - int(warmth_pct / 10))} ë°ìŒ\n       {warmth_pct:.0f}%                {100-warmth_pct:.0f}%\n```")

    with col2:
        # ê°•ì  & ê°œì„ ì 
        st.markdown("**âœ… ê°•ì  (Keep Doing)**")
        strengths = []
        if octaves >= 2.0:
            strengths.append(f"ë„“ì€ ìŒì—­ëŒ€ ({octaves:.1f}ì˜¥íƒ€ë¸Œ) - ë‹¤ì–‘í•œ ê³¡ ì†Œí™” ê°€ëŠ¥")
        # ìŒìƒ‰ì€ íŠ¹ì„±ì´ë¯€ë¡œ ê°ê°ì˜ ì¥ì ì„ ì„¤ëª…
        if warmth_pct >= 50:
            strengths.append("ë”°ëœ»í•œ ìŒìƒ‰ - ë°œë¼ë“œ, ì°¬ì–‘ ì¸ë„ì— ì í•©")
        elif warmth_pct < 30:
            strengths.append("ë°ì€ ìŒìƒ‰ - ì—…í…œí¬, ë°ì€ ì°¬ì–‘ì— ì í•©")
        else:
            strengths.append("ê· í˜• ì¡íŒ ìŒìƒ‰ - ë‹¤ì–‘í•œ ì¥ë¥´ì— ì í•©")
        if vibrato_pct > 30:
            strengths.append("ìì—°ìŠ¤ëŸ¬ìš´ ë¹„ë¸Œë¼í†  - ê°ì • ì „ë‹¬ë ¥ ìš°ìˆ˜")
        if pitch_score >= 70:
            strengths.append("ì•ˆì •ì ì¸ ìŒì • - ì •í™•í•œ í”¼ì¹˜ ì»¨íŠ¸ë¡¤")
        if breath_score >= 70:
            strengths.append("ì¢‹ì€ í˜¸í¡ ì§€ì§€ - í”„ë ˆì´ì¦ˆ ìœ ì§€ë ¥ ìš°ìˆ˜")

        for s in strengths[:4]:
            st.markdown(f"â€¢ {s}")

    st.markdown("---")

    st.markdown("**ğŸ”§ ê°œì„ ì  (Work On)**")

    improvements = []
    if flat_pct > sharp_pct + 10:
        improvements.append(("í”Œë« ê²½í–¥ êµì •", "í”¼ì•„ë…¸/íŠœë„ˆ ì•±ê³¼ í•¨ê»˜ ìŠ¤ì¼€ì¼ ì—°ìŠµ, ìŒì„ ì‚´ì§ ë†’ê²Œ ì¡°ì¤€"))
    if high_stability_pct < 80:
        improvements.append(("ê³ ìŒ ì•ˆì •ì„±", "ë¯¹ìŠ¤ë³´ì´ìŠ¤ í›ˆë ¨, ì„¸ë¯¸ ì˜¤í´ë£¨ì „(ë¹¨ëŒ€ ë°œì„±) ì—°ìŠµ"))
    if breath_phrase < 4.0:
        improvements.append(("í”„ë ˆì´ì¦ˆ ê¸¸ì´", "ë³µì‹í˜¸í¡ ê°•í™”, 30ì´ˆ ì´ìƒ Sustained Tone ì—°ìŠµ"))
    if dynamic_range < 15:
        improvements.append(("ë‹¤ì´ë‚˜ë¯¹ í­ í™•ëŒ€", "pp~ff ê·¹ì  ëŒ€ë¹„ ì—°ìŠµ, ê°ì • ëª°ì…ë„ í–¥ìƒ"))
    if pitch_score < 60:
        improvements.append(("ìŒì • ì •í™•ë„", "íŠœë„ˆ ì•±ìœ¼ë¡œ ì‹¤ì‹œê°„ í”¼ë“œë°± ë°›ìœ¼ë©° ìŠ¤ì¼€ì¼ ì—°ìŠµ"))

    if improvements:
        st.markdown("""
        | ìˆœìœ„ | ê°œì„  ì˜ì—­ | êµ¬ì²´ì  ì—°ìŠµë²• |
        |------|----------|-------------|""")
        for i, (area, method) in enumerate(improvements[:5], 1):
            st.markdown(f"| {i} | {area} | {method} |")
    else:
        st.success("ì „ë°˜ì ìœ¼ë¡œ ìš°ìˆ˜í•œ ë³´ì»¬ ëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ê³  ìˆìŠµë‹ˆë‹¤! í˜„ì¬ ìˆ˜ì¤€ì„ ìœ ì§€í•˜ë©´ì„œ ë‹¤ì–‘í•œ ê³¡ì— ë„ì „í•´ë³´ì„¸ìš”.")


# =============================================
# ì‚¬ì´ë“œë°” - ëª¨ë“œ ì„ íƒ
# =============================================

st.sidebar.title("ğŸ¤ Worship Vocal AI")

analysis_mode = st.sidebar.radio(
    "ë¶„ì„ ëª¨ë“œ",
    ["ğŸµ ë‹¨ì¼ ë¶„ì„", "ğŸ­ ì´ì¤‘ ë¶„ì„ (Dual-Core)"],
    index=0,  # ê¸°ë³¸ê°’: ë‹¨ì¼ ë¶„ì„
    help="ì´ì¤‘ ë¶„ì„: ëŠë¦° ê³¡ + ë¹ ë¥¸ ê³¡ 2ê°œë¥¼ ë¹„êµ ë¶„ì„í•˜ì—¬ ë” ì…ì²´ì ì¸ ë³´ì»¬ í˜ë¥´ì†Œë‚˜ë¥¼ ë„ì¶œí•©ë‹ˆë‹¤."
)

st.sidebar.markdown("---")

if analysis_mode == "ğŸµ ë‹¨ì¼ ë¶„ì„":
    st.sidebar.info("ğŸ’¡ **Tip**: ë¶„ì„ ì™„ë£Œ í›„ 'ì´ì¤‘ ë¶„ì„' ëª¨ë“œë¡œ ì „í™˜í•˜ì—¬ ë‹¤ë¥¸ ìŠ¤íƒ€ì¼ì˜ ê³¡ê³¼ ë¹„êµí•´ë³´ì„¸ìš”!")
else:
    st.sidebar.markdown("""
**ì´ì¤‘ ë¶„ì„ì˜ ì¥ì :**
- ë‘ ìŠ¤íƒ€ì¼ì—ì„œ ê³µí†µë˜ëŠ” ê°•ì /ì•½ì  ë°œê²¬
- ìŠ¤íƒ€ì¼ë³„ ë°˜ì „ ë§¤ë ¥ ë°œê²¬
- ë” ì •í™•í•œ ë³´ì»¬ í˜ë¥´ì†Œë‚˜ ì •ì˜
""")

# =============================================
# P1: ë¶„ì„ íˆìŠ¤í† ë¦¬ (ì„¸ì…˜ ë‚´)
# =============================================
if 'analysis_history' not in st.session_state:
    st.session_state.analysis_history = []

if st.session_state.analysis_history:
    st.sidebar.markdown("---")
    st.sidebar.markdown("### ğŸ“‹ ë¶„ì„ ê¸°ë¡")
    for i, record in enumerate(reversed(st.session_state.analysis_history[-5:])):
        timestamp = record.get('timestamp', '')
        if isinstance(timestamp, datetime):
            timestamp = timestamp.strftime("%H:%M")
        mbti_type = record.get('mbti_type', '?')
        song_title = record.get('song_title', 'ì•Œ ìˆ˜ ì—†ìŒ')[:15]
        st.sidebar.caption(f"{i+1}. {song_title} ({mbti_type}) - {timestamp}")

# =============================================
# P2: íŒ€ì› í”„ë¡œí•„ ì €ì¥ ê¸°ëŠ¥
# =============================================
if 'team_profiles' not in st.session_state:
    st.session_state.team_profiles = {}

st.sidebar.markdown("---")
with st.sidebar.expander("ğŸ‘¥ íŒ€ì› í”„ë¡œí•„ ê´€ë¦¬", expanded=False):
    # ìƒˆ íŒ€ì› ì €ì¥
    if 'analysis_result' in st.session_state and st.session_state.analysis_result:
        new_member_name = st.text_input("íŒ€ì› ì´ë¦„", placeholder="ì˜ˆ: ê¹€ë¯¼ì§€", key="new_member_name")
        if st.button("í˜„ì¬ ë¶„ì„ ê²°ê³¼ ì €ì¥", key="save_profile"):
            if new_member_name:
                result = st.session_state.analysis_result
                st.session_state.team_profiles[new_member_name] = {
                    'mbti_type': result['primary_type'],
                    'vocal_type_name': result['vocal_type_info'].name_kr,
                    'scorecard': result['scorecard'],
                    'strengths': result['vocal_type_info'].strengths,
                    'saved_at': datetime.now().strftime("%Y-%m-%d %H:%M"),
                }
                st.success(f"âœ… {new_member_name} í”„ë¡œí•„ ì €ì¥ë¨!")
                st.rerun()
            else:
                st.warning("íŒ€ì› ì´ë¦„ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")

    # ì €ì¥ëœ íŒ€ì› ëª©ë¡
    if st.session_state.team_profiles:
        st.markdown("##### ì €ì¥ëœ íŒ€ì›")
        for name, profile in st.session_state.team_profiles.items():
            col1, col2 = st.columns([3, 1])
            with col1:
                st.markdown(f"**{name}** ({profile['mbti_type']})")
                st.caption(f"{profile['vocal_type_name']} - {profile['saved_at']}")
            with col2:
                if st.button("ğŸ—‘ï¸", key=f"del_{name}", help="í”„ë¡œí•„ ì‚­ì œ"):
                    del st.session_state.team_profiles[name]
                    st.rerun()
    else:
        st.info("ì €ì¥ëœ íŒ€ì›ì´ ì—†ìŠµë‹ˆë‹¤. ë¶„ì„ í›„ 'í˜„ì¬ ë¶„ì„ ê²°ê³¼ ì €ì¥'ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")

    # P2: íŒ€ì› ë¹„êµ ì°¨íŠ¸ (2ëª… ì´ìƒì¼ ë•Œ)
    if len(st.session_state.team_profiles) >= 2:
        st.markdown("---")
        st.markdown("##### ğŸ“Š íŒ€ì› ë¹„êµ")
        selected_members = st.multiselect(
            "ë¹„êµí•  íŒ€ì› ì„ íƒ",
            options=list(st.session_state.team_profiles.keys()),
            default=list(st.session_state.team_profiles.keys())[:3],
            key="compare_members"
        )

        if len(selected_members) >= 2:
            # ë ˆì´ë” ì°¨íŠ¸ìš© ë°ì´í„° ì¤€ë¹„
            categories = ['ì¹œë°€ê°', 'ë‹¤ì´ë‚˜ë¯¹', 'ìŒìƒ‰', 'ì¸ë„ë ¥', 'ì§€ì†ë ¥', 'í‘œí˜„ë ¥']

            import plotly.graph_objects as go
            fig = go.Figure()

            colors = ['#C9A962', '#7C5CBF', '#4ADE80', '#F87171', '#60A5FA']
            for idx, name in enumerate(selected_members[:5]):  # ìµœëŒ€ 5ëª…
                profile = st.session_state.team_profiles[name]
                scorecard = profile.get('scorecard', {})
                values = [
                    scorecard.get('intimacy', 0.5),
                    scorecard.get('dynamics', 0.5),
                    scorecard.get('tone', 0.5),
                    scorecard.get('leading', 0.5),
                    scorecard.get('sustain', 0.5),
                    scorecard.get('expression', 0.5),
                ]
                values_pct = [v * 100 for v in values]
                values_pct.append(values_pct[0])  # ë‹«ê¸°

                fig.add_trace(go.Scatterpolar(
                    r=values_pct,
                    theta=categories + [categories[0]],
                    fill='toself',
                    name=f"{name} ({profile['mbti_type']})",
                    line_color=colors[idx % len(colors)],
                    opacity=0.7
                ))

            fig.update_layout(
                polar=dict(
                    radialaxis=dict(visible=True, range=[0, 100]),
                    bgcolor='rgba(0,0,0,0)'
                ),
                showlegend=True,
                legend=dict(orientation="h", yanchor="bottom", y=-0.3),
                paper_bgcolor='rgba(0,0,0,0)',
                plot_bgcolor='rgba(0,0,0,0)',
                height=300,
                margin=dict(l=30, r=30, t=30, b=50)
            )
            st.plotly_chart(fig, use_container_width=True)

            # íŒ€ ìš”ì•½
            st.caption(f"ğŸ¯ ì´ {len(st.session_state.team_profiles)}ëª…ì˜ íŒ€ì› í”„ë¡œí•„ ì €ì¥ë¨")

# =============================================
# ë©”ì¸ í—¤ë”
# =============================================

st.title("ğŸ¤ Worship Vocal AI Coach")

# =============================================
# P0: ì²« ì‚¬ìš©ì ì˜¨ë³´ë”© ê°€ì´ë“œ
# =============================================
if 'first_visit' not in st.session_state:
    st.session_state.first_visit = True

if st.session_state.first_visit:
    with st.expander("ğŸ‰ ì²˜ìŒ ì˜¤ì…¨ë‚˜ìš”? ì‹œì‘ ê°€ì´ë“œ", expanded=True):
        st.markdown("""
### 3ë‹¨ê³„ë¡œ ë³´ì»¬ ë¶„ì„ ì™„ë£Œ!

**1ï¸âƒ£ ì°¬ì–‘ ì—…ë¡œë“œ** - YouTube ë§í¬ ë˜ëŠ” ë…¹ìŒ íŒŒì¼
**2ï¸âƒ£ ë…¹ìŒ í™˜ê²½ ì„ íƒ** - ì†”ë¡œ? ë°˜ì£¼ì™€ í•¨ê»˜?
**3ï¸âƒ£ AI ë¶„ì„ ì‹œì‘** - 1-5ë¶„ í›„ ê²°ê³¼ í™•ì¸!

> ğŸ’¡ **íŒ**: 'ì´ì¤‘ ë¶„ì„'ì€ ëŠë¦° ê³¡ + ë¹ ë¥¸ ê³¡ 2ê°œë¥¼ ë¹„êµí•´ì„œ ë” ì •í™•í•œ ë³´ì»¬ ìŠ¤íƒ€ì¼ì„ ì•Œë ¤ì¤˜ìš”!

---

**ê²°ê³¼ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆëŠ” ê²ƒ:**
- ğŸ­ **ë³´ì»¬ MBTI** - 8ê°€ì§€ ìœ í˜• ì¤‘ ë‹¹ì‹ ì˜ ìŠ¤íƒ€ì¼
- ğŸ“Š **ê¸°ìˆ  ë¶„ì„** - ìŒì •, ìŒìƒ‰, ë‹¤ì´ë‚˜ë¯¹ ë“± ìƒì„¸ ë¶„ì„
- ğŸµ **ì¶”ì²œ ì°¬ì–‘** - ë‹¹ì‹ ì—ê²Œ ì–´ìš¸ë¦¬ëŠ” ì°¬ì–‘ ë¦¬ìŠ¤íŠ¸
- ğŸ“¥ **PDF/ì´ë¯¸ì§€** - ë¶„ì„ ê²°ê³¼ ì €ì¥ ë° ê³µìœ 
        """)
        if st.button("ì•Œê² ì–´ìš”, ì‹œì‘í• ê²Œìš”!", type="primary"):
            st.session_state.first_visit = False
            st.rerun()

if analysis_mode == "ğŸ­ ì´ì¤‘ ë¶„ì„ (Dual-Core)":
    st.markdown("""
    **ì´ì¤‘ ë¶„ì„ ëª¨ë“œ**: ì„œë¡œ ë‹¤ë¥¸ ìŠ¤íƒ€ì¼ì˜ 2ê³¡ì„ ë¹„êµ ë¶„ì„í•˜ì—¬ **ì…ì²´ì ì¸ ë³´ì»¬ í˜ë¥´ì†Œë‚˜**ë¥¼ ë„ì¶œí•©ë‹ˆë‹¤.
    """)
else:
    st.markdown("ë‹¹ì‹ ì˜ ì°¬ì–‘ì„ ë¶„ì„í•˜ê³ , **ë³´ì»¬ MBTI**ì™€ **ë§ì¶¤ ì½”ì¹­**ì„ ì œê³µí•©ë‹ˆë‹¤.")


# =============================================
# ì´ì¤‘ ë¶„ì„ ëª¨ë“œ
# =============================================

if analysis_mode == "ğŸ­ ì´ì¤‘ ë¶„ì„ (Dual-Core)":

    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if 'mission_a_path' not in st.session_state:
        st.session_state.mission_a_path = None
    if 'mission_b_path' not in st.session_state:
        st.session_state.mission_b_path = None
    if 'dual_result' not in st.session_state:
        st.session_state.dual_result = None

    # ë…¹ìŒ í™˜ê²½ ì„¤ì • (ì´ì¤‘ ë¶„ì„ìš©)
    st.subheader("ğŸ¤ ë…¹ìŒ í™˜ê²½")
    dual_recording_type = st.selectbox(
        "ë…¹ìŒ ìƒí™©ì„ ì„ íƒí•˜ì„¸ìš”",
        [
            "ğŸ¤ ì†”ë¡œ ë…¹ìŒ (ë‚˜ë§Œ ë…¹ìŒë¨)",
            "ğŸ¹ ë°˜ì£¼ì™€ í•¨ê»˜ (MR + ë‚´ ëª©ì†Œë¦¬)",
            "ğŸ‘¥ ì°¬ì–‘íŒ€ê³¼ í•¨ê»˜ (ë‚´ê°€ ë©”ì¸ ì¸ë„ì)"
        ],
        key="dual_recording_type"
    )
    dual_need_separation = dual_recording_type != "ğŸ¤ ì†”ë¡œ ë…¹ìŒ (ë‚˜ë§Œ ë…¹ìŒë¨)"

    if dual_need_separation:
        st.info("ğŸ”§ AIê°€ ìë™ìœ¼ë¡œ ë³´ì»¬ì„ ë¶„ë¦¬í•˜ì—¬ ë¶„ì„í•©ë‹ˆë‹¤.")

    st.markdown("---")

    # Song A ì…ë ¥
    st.header("ğŸµ Song A: ì²« ë²ˆì§¸ ê³¡")

    col_a1, col_a2 = st.columns([2, 1])

    with col_a1:
        input_method_a = st.radio(
            "ì…ë ¥ ë°©ì‹ (Mission A)",
            ["ğŸ”— YouTube ë§í¬", "ğŸ“ íŒŒì¼ ì—…ë¡œë“œ"],
            horizontal=True,
            key="input_a"
        )

    with col_a2:
        song_title_a = st.text_input("ê³¡ ì œëª© (Song A)", placeholder="ì˜ˆ: ë‚˜ ë¬´ë ¥í• ìˆ˜ë¡", key="title_a")

    if input_method_a == "ğŸ”— YouTube ë§í¬":
        url_a = st.text_input("YouTube URL (Mission A)", placeholder="https://www.youtube.com/watch?v=...", key="url_a")

        st.caption("âœ‚ï¸ ë¶„ì„í•  êµ¬ê°„ (MM:SS í˜•ì‹)")
        col1, col2, col3 = st.columns([2, 2, 1])
        with col1:
            start_a = st.text_input("â±ï¸ ì‹œì‘", "0:00", key="start_a")
        with col2:
            end_a = st.text_input("â±ï¸ ì¢…ë£Œ", "", key="end_a", help="ë¹„ì›Œë‘ë©´ ëê¹Œì§€")
        with col3:
            if start_a and end_a:
                try:
                    s = time_to_seconds(start_a) or 0
                    e = time_to_seconds(end_a)
                    if e and e > s:
                        d = e - s
                        st.metric("ê¸¸ì´", f"{d//60}:{d%60:02d}")
                except:
                    pass

        if st.button("ğŸµ Mission A ì¶”ì¶œ", key="extract_a"):
            if url_a:
                with st.spinner("Mission A ì˜¤ë””ì˜¤ ì¶”ì¶œ ì¤‘..."):
                    try:
                        path, title = extract_youtube_audio(url_a, start_a, end_a, "mission_a")
                        st.session_state.mission_a_path = path
                        st.session_state.mission_a_title = title
                        st.success(f"âœ… Mission A ì¶”ì¶œ ì™„ë£Œ! ({title})")
                        st.audio(path)
                    except Exception as e:
                        st.error(f"ì¶”ì¶œ ì‹¤íŒ¨: {e}")
    else:
        uploaded_a = st.file_uploader("ì˜¤ë””ì˜¤ íŒŒì¼ (Mission A)", type=['mp3', 'wav', 'm4a'], key="file_a")
        if uploaded_a:
            temp_path = f"/tmp/mission_a_{uploaded_a.name}"
            with open(temp_path, "wb") as f:
                f.write(uploaded_a.getbuffer())
            st.session_state.mission_a_path = temp_path
            st.audio(uploaded_a)

    st.markdown("---")

    # Song B ì…ë ¥
    st.header("ğŸµ Song B: ë‘ ë²ˆì§¸ ê³¡")

    col_b1, col_b2 = st.columns([2, 1])

    with col_b1:
        input_method_b = st.radio(
            "ì…ë ¥ ë°©ì‹ (Mission B)",
            ["ğŸ”— YouTube ë§í¬", "ğŸ“ íŒŒì¼ ì—…ë¡œë“œ"],
            horizontal=True,
            key="input_b"
        )

    with col_b2:
        song_title_b = st.text_input("ê³¡ ì œëª© (Mission B)", placeholder="ì˜ˆ: ì‚´ì•„ê³„ì‹  ì£¼", key="title_b")

    if input_method_b == "ğŸ”— YouTube ë§í¬":
        url_b = st.text_input("YouTube URL (Mission B)", placeholder="https://www.youtube.com/watch?v=...", key="url_b")

        st.caption("âœ‚ï¸ ë¶„ì„í•  êµ¬ê°„ (MM:SS í˜•ì‹)")
        col1, col2, col3 = st.columns([2, 2, 1])
        with col1:
            start_b = st.text_input("â±ï¸ ì‹œì‘", "0:00", key="start_b")
        with col2:
            end_b = st.text_input("â±ï¸ ì¢…ë£Œ", "", key="end_b", help="ë¹„ì›Œë‘ë©´ ëê¹Œì§€")
        with col3:
            if start_b and end_b:
                try:
                    s = time_to_seconds(start_b) or 0
                    e = time_to_seconds(end_b)
                    if e and e > s:
                        d = e - s
                        st.metric("ê¸¸ì´", f"{d//60}:{d%60:02d}")
                except:
                    pass

        if st.button("ğŸµ Mission B ì¶”ì¶œ", key="extract_b"):
            if url_b:
                with st.spinner("Mission B ì˜¤ë””ì˜¤ ì¶”ì¶œ ì¤‘..."):
                    try:
                        path, title = extract_youtube_audio(url_b, start_b, end_b, "mission_b")
                        st.session_state.mission_b_path = path
                        st.session_state.mission_b_title = title
                        st.success(f"âœ… Mission B ì¶”ì¶œ ì™„ë£Œ! ({title})")
                        st.audio(path)
                    except Exception as e:
                        st.error(f"ì¶”ì¶œ ì‹¤íŒ¨: {e}")
    else:
        uploaded_b = st.file_uploader("ì˜¤ë””ì˜¤ íŒŒì¼ (Mission B)", type=['mp3', 'wav', 'm4a'], key="file_b")
        if uploaded_b:
            temp_path = f"/tmp/mission_b_{uploaded_b.name}"
            with open(temp_path, "wb") as f:
                f.write(uploaded_b.getbuffer())
            st.session_state.mission_b_path = temp_path
            st.audio(uploaded_b)

    st.markdown("---")

    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (ë¶„ë¦¬ ê²°ê³¼ ì €ì¥ìš©)
    if 'separated_vocals_a' not in st.session_state:
        st.session_state.separated_vocals_a = None
    if 'separated_vocals_b' not in st.session_state:
        st.session_state.separated_vocals_b = None
    if 'separated_instrumental_a' not in st.session_state:
        st.session_state.separated_instrumental_a = None
    if 'separated_instrumental_b' not in st.session_state:
        st.session_state.separated_instrumental_b = None

    # ì´ì¤‘ ë¶„ì„ ì‹¤í–‰ (2ë‹¨ê³„ ë¶„ë¦¬)
    if st.session_state.mission_a_path and st.session_state.mission_b_path:

        # ========== STEP 1: ë³´ì»¬ ë¶„ë¦¬ ==========
        if dual_need_separation and not st.session_state.separated_vocals_a:
            st.subheader("ğŸ“Œ Step 1: ë³´ì»¬ ë¶„ë¦¬")

            if st.button("ğŸ­ ë³´ì»¬ ë¶„ë¦¬ ì‹œì‘", type="primary", key="step1_separate"):
                progress = st.progress(0)
                status = st.empty()

                try:
                    from vocal_separator import auto_separate, SeparationMode

                    audio_path_a = st.session_state.mission_a_path
                    audio_path_b = st.session_state.mission_b_path

                    # íŒŒì¼ í¬ê¸° í™•ì¸
                    size_a = os.path.getsize(audio_path_a) / (1024 * 1024)
                    size_b = os.path.getsize(audio_path_b) / (1024 * 1024)
                    total_size = size_a + size_b
                    if total_size > 50:
                        est_time = int((total_size * 18 + 300) / 60)
                        st.warning(f"â±ï¸ íŒŒì¼ í¬ê¸°ê°€ í½ë‹ˆë‹¤ (ì´ {total_size:.0f}MB). ë³´ì»¬ ë¶„ë¦¬ì— ì•½ {est_time}ë¶„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

                    status.text(f"ğŸ­ Song A ë³´ì»¬ ë¶„ë¦¬ ì¤‘... ({size_a:.0f}MB)")
                    sep_result_a = auto_separate(audio_path_a, "/tmp/separated_a", mode=SeparationMode.VOCALS_ONLY)
                    progress.progress(40)

                    status.text(f"ğŸ­ Song B ë³´ì»¬ ë¶„ë¦¬ ì¤‘... ({size_b:.0f}MB)")
                    sep_result_b = auto_separate(audio_path_b, "/tmp/separated_b", mode=SeparationMode.VOCALS_ONLY)
                    progress.progress(80)

                    # ê²°ê³¼ ì €ì¥
                    if sep_result_a.success and sep_result_a.lead_vocals_path:
                        st.session_state.separated_vocals_a = sep_result_a.lead_vocals_path
                        st.session_state.separated_instrumental_a = sep_result_a.instrumental_path
                    if sep_result_b.success and sep_result_b.lead_vocals_path:
                        st.session_state.separated_vocals_b = sep_result_b.lead_vocals_path
                        st.session_state.separated_instrumental_b = sep_result_b.instrumental_path

                    progress.progress(100)
                    status.text("âœ… ë³´ì»¬ ë¶„ë¦¬ ì™„ë£Œ!")
                    st.rerun()

                except Exception as e:
                    st.error(f"ë¶„ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

        # ========== ë¶„ë¦¬ ì™„ë£Œ í›„: ë¯¸ë¦¬ë“£ê¸° & ë‹¤ìš´ë¡œë“œ ==========
        if st.session_state.separated_vocals_a and st.session_state.separated_vocals_b:
            st.success("âœ… ë³´ì»¬ ë¶„ë¦¬ ì™„ë£Œ! ì•„ë˜ì—ì„œ ë¯¸ë¦¬ ë“£ê±°ë‚˜ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

            # ë¯¸ë¦¬ë“£ê¸° ì„¹ì…˜
            st.subheader("ğŸ§ ë¶„ë¦¬ëœ ë³´ì»¬ ë¯¸ë¦¬ë“£ê¸° & ë‹¤ìš´ë¡œë“œ")
            col_prev1, col_prev2 = st.columns(2)

            with col_prev1:
                st.markdown(f"**ğŸµ {song_title_a or 'Song A'} - ë³´ì»¬**")
                if os.path.exists(st.session_state.separated_vocals_a):
                    st.audio(st.session_state.separated_vocals_a, format='audio/wav')
                    with open(st.session_state.separated_vocals_a, 'rb') as f:
                        st.download_button("â¬‡ï¸ ë³´ì»¬ ë‹¤ìš´ë¡œë“œ", f, f"{song_title_a or 'SongA'}_vocals.wav", "audio/wav", key="dl_voc_a")
                if st.session_state.separated_instrumental_a and os.path.exists(st.session_state.separated_instrumental_a):
                    st.markdown("**ğŸ¸ MR (ë°˜ì£¼)**")
                    st.audio(st.session_state.separated_instrumental_a, format='audio/wav')
                    with open(st.session_state.separated_instrumental_a, 'rb') as f:
                        st.download_button("â¬‡ï¸ MR ë‹¤ìš´ë¡œë“œ", f, f"{song_title_a or 'SongA'}_mr.wav", "audio/wav", key="dl_mr_a")

            with col_prev2:
                st.markdown(f"**ğŸµ {song_title_b or 'Song B'} - ë³´ì»¬**")
                if os.path.exists(st.session_state.separated_vocals_b):
                    st.audio(st.session_state.separated_vocals_b, format='audio/wav')
                    with open(st.session_state.separated_vocals_b, 'rb') as f:
                        st.download_button("â¬‡ï¸ ë³´ì»¬ ë‹¤ìš´ë¡œë“œ", f, f"{song_title_b or 'SongB'}_vocals.wav", "audio/wav", key="dl_voc_b")
                if st.session_state.separated_instrumental_b and os.path.exists(st.session_state.separated_instrumental_b):
                    st.markdown("**ğŸ¸ MR (ë°˜ì£¼)**")
                    st.audio(st.session_state.separated_instrumental_b, format='audio/wav')
                    with open(st.session_state.separated_instrumental_b, 'rb') as f:
                        st.download_button("â¬‡ï¸ MR ë‹¤ìš´ë¡œë“œ", f, f"{song_title_b or 'SongB'}_mr.wav", "audio/wav", key="dl_mr_b")

            st.markdown("---")

            # ========== STEP 2: í”¼ì¹˜ ë¶„ì„ ==========
            st.subheader("ğŸ“Œ Step 2: ë³´ì»¬ ë¶„ì„")
            st.caption("ë¶„ë¦¬ëœ ë³´ì»¬ì„ ë¶„ì„í•˜ì—¬ ê¸°ìˆ  ì ìˆ˜, ìŠ¤íƒ€ì¼, MBTI ë“±ì„ ê³„ì‚°í•©ë‹ˆë‹¤.")

            if st.button("ğŸ” ë¶„ì„ ê³„ì†í•˜ê¸°", type="primary", key="step2_analyze"):
                progress = st.progress(0)
                status = st.empty()

                try:
                    audio_path_a = st.session_state.separated_vocals_a
                    audio_path_b = st.session_state.separated_vocals_b

                    # Song A ë¶„ì„ (ì‹œê³„ì—´ í¬í•¨)
                    status.text("ğŸ“Š Song A ë¶„ì„ ì¤‘...")
                    features_a = analyze_audio_features(audio_path_a, include_timeseries=True)
                    progress.progress(30)

                    # Song B ë¶„ì„ (ì‹œê³„ì—´ í¬í•¨)
                    status.text("ğŸ“Š Song B ë¶„ì„ ì¤‘...")
                    features_b = analyze_audio_features(audio_path_b, include_timeseries=True)
                    progress.progress(60)

                    # LLM ê¸°ë°˜ ë¶„ì„ ì‚¬ìš©
                    status.text("ğŸ¤– AI(Claude) ë¶„ì„ ì¤‘...")

                    from llm_analyzer import analyze_with_llm

                    llm_result = analyze_with_llm(
                        features_a,
                        features_b,
                        song_title_a or "Song A",
                        song_title_b or "Song B"
                    )

                    progress.progress(85)

                    # ë ˆì´ë” ì°¨íŠ¸ ë° DNA ê³„ì‚° (ê°œì„ ëœ ì ìˆ˜ ì‹œìŠ¤í…œ)
                    avg_dynamic_score = (features_a['dynamic_score'] + features_b['dynamic_score']) / 2
                    avg_breath_score = (features_a['breath_support_score'] + features_b['breath_support_score']) / 2
                    avg_centroid = (features_a['spectral_centroid_hz'] + features_b['spectral_centroid_hz']) / 2
                    avg_stability = (features_a['high_note_stability'] + features_b['high_note_stability']) / 2
                    avg_clarity = (features_a['articulation_clarity'] + features_b['articulation_clarity']) / 2
                    avg_rhythm = (features_a['rhythm_offset_ms'] + features_b['rhythm_offset_ms']) / 2

                    radar_stats = {
                        "ê°ì„±": avg_dynamic_score * 100,  # ê°œì„ : ìµœì  ë²”ìœ„ ë°˜ì˜
                        "ìŒìƒ‰": min(100, 100 - abs(avg_centroid - 1800) / 20),
                        "ë¦¬ë“¬": min(100, 100 - avg_rhythm),
                        "ë°œì„±": avg_stability * 100,
                        "ë¦¬ë”©": avg_clarity * 100
                    }

                    vocal_dna = {
                        "ë”°ëœ»í•¨": max(0, min(100, (3000 - avg_centroid) / 15)),
                        "íŒŒì›Œ": avg_dynamic_score * 100,  # ê°œì„ : ìµœì  ë²”ìœ„ ë°˜ì˜
                        "ì•ˆì •ì„±": avg_stability * 100,
                        "í‘œí˜„ë ¥": avg_dynamic_score * 80 + avg_breath_score * 20,  # ë‹¤ì´ë‚˜ë¯¹ + í˜¸í¡
                        "ê·¸ë£¨ë¸Œ": max(0, min(100, 100 - avg_rhythm)),
                        "ì¹œë°€ê°": avg_breath_score * 100  # ê°œì„ : í˜¸í¡ ì§€ì§€ ê¸°ë°˜
                    }

                    # ê²°ê³¼ ê°ì²´ ìƒì„± (LLM ê²°ê³¼ + ê³„ì‚°ëœ ì°¨íŠ¸ ë°ì´í„°)
                    class LLMDualResult:
                        pass

                    result = LLMDualResult()
                    result.persona_name = llm_result.persona_name
                    result.persona_icon = llm_result.persona_icon
                    result.persona_description = llm_result.persona_description
                    result.signature_name = llm_result.signature_name
                    result.signature_description = llm_result.signature_description
                    result.signature_evidence = llm_result.signature_evidence
                    result.enemy_name = llm_result.enemy_name
                    result.enemy_description = llm_result.enemy_description
                    result.enemy_evidence = llm_result.enemy_evidence
                    result.solution = llm_result.solution
                    result.exercise = llm_result.exercise
                    result.vocal_mbti = llm_result.vocal_mbti
                    result.mbti_reason = llm_result.mbti_reason
                    result.overall_assessment = llm_result.overall_assessment
                    result.matching_songs = llm_result.matching_songs
                    result.challenge_songs = llm_result.challenge_songs
                    result.radar_stats = radar_stats
                    result.vocal_dna = vocal_dna

                    # ê³¡ ì •ë³´ (í‘œì‹œìš©)
                    class SongInfo:
                        pass
                    result.slow_song = SongInfo()
                    result.slow_song.song_title = song_title_a or "Song A"
                    result.fast_song = SongInfo()
                    result.fast_song.song_title = song_title_b or "Song B"

                    progress.progress(90)

                    # ê²°ê³¼ ì €ì¥
                    st.session_state.dual_result = {
                        'result': result,
                        'features_a': features_a,
                        'features_b': features_b
                    }

                    # ë³´ì»¬ ë¶„ë¦¬ ê²°ê³¼ ì €ì¥ (ë‹¤ìš´ë¡œë“œìš©) - ì´ë¯¸ ì„¸ì…˜ì— ì €ì¥ë¨
                    st.session_state.dual_separation_result = {
                        'song_a': {
                            'vocals_path': st.session_state.separated_vocals_a,
                            'instrumental_path': st.session_state.separated_instrumental_a,
                            'confidence': 0.9,
                            'title': song_title_a or "Song A"
                        },
                        'song_b': {
                            'vocals_path': st.session_state.separated_vocals_b,
                            'instrumental_path': st.session_state.separated_instrumental_b,
                            'confidence': 0.9,
                            'title': song_title_b or "Song B"
                        }
                    }

                    progress.progress(100)
                    status.text("âœ… ì´ì¤‘ ë¶„ì„ ì™„ë£Œ!")

                except Exception as e:
                    st.error(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    import traceback
                    st.code(traceback.format_exc())

        # ========== ë¶„ë¦¬ í•„ìš” ì—†ëŠ” ê²½ìš°: ë°”ë¡œ ë¶„ì„ ==========
        elif not dual_need_separation:
            st.subheader("ğŸ“Œ ë³´ì»¬ ë¶„ì„")
            st.caption("íŒŒì¼ì—ì„œ ì§ì ‘ ë³´ì»¬ì„ ë¶„ì„í•©ë‹ˆë‹¤.")

            if st.button("ğŸ” ë¶„ì„ ì‹œì‘", type="primary", key="direct_analyze"):
                progress = st.progress(0)
                status = st.empty()

                try:
                    audio_path_a = st.session_state.mission_a_path
                    audio_path_b = st.session_state.mission_b_path

                    # Song A ë¶„ì„ (ì‹œê³„ì—´ í¬í•¨)
                    status.text("ğŸ“Š Song A ë¶„ì„ ì¤‘...")
                    features_a = analyze_audio_features(audio_path_a, include_timeseries=True)
                    progress.progress(30)

                    # Song B ë¶„ì„ (ì‹œê³„ì—´ í¬í•¨)
                    status.text("ğŸ“Š Song B ë¶„ì„ ì¤‘...")
                    features_b = analyze_audio_features(audio_path_b, include_timeseries=True)
                    progress.progress(60)

                    # LLM ê¸°ë°˜ ë¶„ì„ ì‚¬ìš©
                    status.text("ğŸ¤– AI(Claude) ë¶„ì„ ì¤‘...")

                    from llm_analyzer import analyze_with_llm

                    llm_result = analyze_with_llm(
                        features_a,
                        features_b,
                        song_title_a or "Song A",
                        song_title_b or "Song B"
                    )

                    progress.progress(85)

                    # ë ˆì´ë” ì°¨íŠ¸ ë° DNA ê³„ì‚°
                    avg_dynamic_score = (features_a['dynamic_score'] + features_b['dynamic_score']) / 2
                    avg_breath_score = (features_a['breath_support_score'] + features_b['breath_support_score']) / 2
                    avg_centroid = (features_a['spectral_centroid_hz'] + features_b['spectral_centroid_hz']) / 2
                    avg_stability = (features_a['high_note_stability'] + features_b['high_note_stability']) / 2
                    avg_clarity = (features_a['articulation_clarity'] + features_b['articulation_clarity']) / 2
                    avg_rhythm = (features_a['rhythm_offset_ms'] + features_b['rhythm_offset_ms']) / 2

                    radar_stats = {
                        "ê°ì„±": avg_dynamic_score * 100,
                        "ìŒìƒ‰": min(100, 100 - abs(avg_centroid - 1800) / 20),
                        "ë¦¬ë“¬": min(100, 100 - avg_rhythm),
                        "ë°œì„±": avg_stability * 100,
                        "ë¦¬ë”©": avg_clarity * 100
                    }

                    vocal_dna = {
                        "ë”°ëœ»í•¨": max(0, min(100, (3000 - avg_centroid) / 15)),
                        "íŒŒì›Œ": avg_dynamic_score * 100,
                        "ì•ˆì •ì„±": avg_stability * 100,
                        "í‘œí˜„ë ¥": avg_dynamic_score * 80 + avg_breath_score * 20,
                        "ê·¸ë£¨ë¸Œ": max(0, min(100, 100 - avg_rhythm)),
                        "ì¹œë°€ê°": avg_breath_score * 100
                    }

                    class LLMDualResult:
                        pass

                    result = LLMDualResult()
                    result.persona_name = llm_result.persona_name
                    result.persona_icon = llm_result.persona_icon
                    result.persona_description = llm_result.persona_description
                    result.signature_name = llm_result.signature_name
                    result.signature_description = llm_result.signature_description
                    result.signature_evidence = llm_result.signature_evidence
                    result.enemy_name = llm_result.enemy_name
                    result.enemy_description = llm_result.enemy_description
                    result.enemy_evidence = llm_result.enemy_evidence
                    result.solution = llm_result.solution
                    result.exercise = llm_result.exercise
                    result.vocal_mbti = llm_result.vocal_mbti
                    result.mbti_reason = llm_result.mbti_reason
                    result.overall_assessment = llm_result.overall_assessment
                    result.matching_songs = llm_result.matching_songs
                    result.challenge_songs = llm_result.challenge_songs
                    result.radar_stats = radar_stats
                    result.vocal_dna = vocal_dna

                    class SongInfo:
                        pass
                    result.slow_song = SongInfo()
                    result.slow_song.song_title = song_title_a or "Song A"
                    result.fast_song = SongInfo()
                    result.fast_song.song_title = song_title_b or "Song B"

                    # ê²°ê³¼ ì €ì¥
                    st.session_state.dual_result = {
                        'result': result,
                        'features_a': features_a,
                        'features_b': features_b
                    }
                    st.session_state.dual_separation_result = None

                    progress.progress(100)
                    status.text("âœ… ë¶„ì„ ì™„ë£Œ!")

                except Exception as e:
                    st.error(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    import traceback
                    st.code(traceback.format_exc())

    # ê²°ê³¼ í‘œì‹œ
    if st.session_state.dual_result:
        result = st.session_state.dual_result['result']
        features_a = st.session_state.dual_result['features_a']
        features_b = st.session_state.dual_result['features_b']

        st.markdown("---")
        st.header("ğŸ­ ì´ì¤‘ ë¶„ì„ ê²°ê³¼")

        # íƒ­ ì¸í„°í˜ì´ìŠ¤
        tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs(["ğŸ­ ë³´ì»¬ ì½”ì¹­", "ğŸ“Š Song A ê¸°ìˆ  ë¶„ì„", "ğŸ“Š Song B ê¸°ìˆ  ë¶„ì„", "ğŸµ ì¶”ì²œ ì°¬ì–‘", "ğŸ“‹ ë³´ì»¬ MBTI ìœ í˜•", "ğŸ“¥ ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ"])

        with tab1:
            # í˜ë¥´ì†Œë‚˜ ì¹´ë“œ
            st.subheader(f"{result.persona_icon} THE PERSONA: {result.persona_name}")
            st.info(result.persona_description)

            # 2ì—´ ë ˆì´ì•„ì›ƒ (SIGNATURE / HIDDEN ENEMY)
            col1, col2 = st.columns(2)

            with col1:
                st.markdown("### â­ YOUR SIGNATURE")
                st.markdown(f"**{result.signature_name}**")
                st.write(result.signature_description)
                if result.signature_evidence:
                    # JSON ëŒ€ì‹  í…Œì´ë¸”ë¡œ í‘œì‹œ
                    st.markdown("**ğŸ“Š ê·¼ê±°:**")
                    for key, value in result.signature_evidence.items():
                        display_key = key.replace('_', ' ').replace('song a', 'Song A').replace('song b', 'Song B')
                        st.write(f"- {display_key}: **{value}**")

            with col2:
                st.markdown("### ğŸ¯ HIDDEN ENEMY")
                st.markdown(f"**{result.enemy_name}**")
                st.write(result.enemy_description)
                if result.enemy_evidence:
                    st.markdown("**ğŸ“Š ê·¼ê±°:**")
                    for key, value in result.enemy_evidence.items():
                        display_key = key.replace('_', ' ').replace('song a', 'Song A').replace('song b', 'Song B')
                        if isinstance(value, float):
                            st.write(f"- {display_key}: **{value:.1f}**")
                        else:
                            st.write(f"- {display_key}: **{value}**")

            st.markdown("---")

            # VOCAL IDENTITY (ìƒì„¸í™” - LLM ì´ìœ  í¬í•¨)
            from vocal_mbti import VOCAL_TYPES
            st.markdown("### ğŸ§¬ VOCAL IDENTITY")

            id_col1, id_col2 = st.columns([1, 2])

            with id_col1:
                st.metric("MBTI íƒ€ì…", result.vocal_mbti)

            with id_col2:
                if result.vocal_mbti in VOCAL_TYPES:
                    vtype = VOCAL_TYPES[result.vocal_mbti]
                    st.markdown(f"**{vtype.name_en}** ({vtype.name_kr})")
                    st.write(vtype.description)
                    st.markdown("**ë¡¤ëª¨ë¸:** " + ", ".join(vtype.role_models))

                # LLMì´ ë¶„ì„í•œ ì´ìœ  í‘œì‹œ
                if hasattr(result, 'mbti_reason') and result.mbti_reason:
                    st.info(f"**AI ë¶„ì„:** {result.mbti_reason}")

            st.markdown("---")

            # ì°¬ì–‘ ì˜ˆë°° ìŠ¤íƒ€ì¼ (í‰ê°€ë³´ë‹¤ ìŠ¤íƒ€ì¼ ì•ˆë‚´)
            st.markdown("### â›ª ì°¬ì–‘ ì˜ˆë°° ìŠ¤íƒ€ì¼")
            from worship_style import calculate_worship_style, WORSHIP_STYLE_AXES, StyleDimension

            # ë‘ ê³¡ì˜ í‰ê·  íŠ¹ì„±ìœ¼ë¡œ ìŠ¤íƒ€ì¼ ê³„ì‚°
            avg_features = {
                'dynamic_score': (features_a.get('dynamic_score', 0.5) + features_b.get('dynamic_score', 0.5)) / 2,
                'warmth_score': (features_a.get('warmth_score', 0.5) + features_b.get('warmth_score', 0.5)) / 2,
                'high_note_stability': (features_a.get('high_note_stability', 0.5) + features_b.get('high_note_stability', 0.5)) / 2,
                'breath_support_score': (features_a.get('breath_support_score', 0.5) + features_b.get('breath_support_score', 0.5)) / 2,
                'energy_variance': (features_a.get('energy_variance', 0.1) + features_b.get('energy_variance', 0.1)) / 2,
                'vibrato_ratio': (features_a.get('vibrato_ratio', 0.3) + features_b.get('vibrato_ratio', 0.3)) / 2,
            }
            worship_style = calculate_worship_style(avg_features)

            # ìŠ¤íƒ€ì¼ ì´ë¦„ê³¼ ì„¤ëª…
            st.success(f"{worship_style.icon} **{worship_style.style_name}** ({worship_style.style_name_en})")
            st.write(worship_style.description)

            # ìŠ¤íƒ€ì¼ ì°¨ì› ì‹œê°í™”
            style_col1, style_col2 = st.columns(2)

            with style_col1:
                st.markdown("**âœ¨ ê°•ì :**")
                for strength in worship_style.strengths:
                    st.write(f"â€¢ {strength}")

            with style_col2:
                st.markdown("**â›ª ì–´ìš¸ë¦¬ëŠ” ì˜ˆë°°:**")
                for context in worship_style.best_fit_contexts:
                    st.write(f"â€¢ {context}")

            # ìŠ¤íƒ€ì¼ ì¶• í‘œì‹œ (expander)
            with st.expander("ğŸ“Š ìŠ¤íƒ€ì¼ ìƒì„¸ ë¶„ì„"):
                for dim, score in worship_style.dimension_scores.items():
                    axis = WORSHIP_STYLE_AXES[dim]
                    # ìŠ¤íƒ€ì¼ ë°” í‘œì‹œ
                    st.write(f"**{axis.low_icon} {axis.low_label}** â† â†’ **{axis.high_label} {axis.high_icon}**")
                    st.progress(float(score))
                    if score < 0.35:
                        st.caption(f"â†’ {axis.worship_context_low}")
                    elif score > 0.65:
                        st.caption(f"â†’ {axis.worship_context_high}")
                    else:
                        st.caption("â†’ ë‹¤ì–‘í•œ ìƒí™©ì— ìœ ì—°í•˜ê²Œ ì ì‘")

            # ğŸ“± SNS ê³µìœ  ì´ë¯¸ì§€ ìƒì„±
            with st.expander("ğŸ“± SNS ê³µìœ  ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ"):
                st.caption("í˜ë¥´ì†Œë‚˜ ì¹´ë“œë¥¼ ì´ë¯¸ì§€ë¡œ ì €ì¥í•˜ì—¬ SNSì— ê³µìœ í•˜ì„¸ìš”!")
                share_col1, share_col2 = st.columns(2)

                with share_col1:
                    if st.button("ğŸ“¥ ìŠ¤í† ë¦¬ìš© (9:16)", key="share_story_dual"):
                        try:
                            from components.share_image import create_persona_card_image

                            dim_scores_str = {
                                dim.value if hasattr(dim, 'value') else str(dim): score
                                for dim, score in worship_style.dimension_scores.items()
                            }

                            img_bytes = create_persona_card_image(
                                style_name=worship_style.style_name,
                                style_name_en=worship_style.style_name_en,
                                icon=worship_style.icon,
                                description=worship_style.description,
                                strengths=worship_style.strengths,
                                best_fit_contexts=worship_style.best_fit_contexts,
                                dimension_scores=dim_scores_str
                            )

                            st.download_button(
                                label="ğŸ’¾ ì´ë¯¸ì§€ ì €ì¥",
                                data=img_bytes,
                                file_name="worship_vocal_persona.png",
                                mime="image/png",
                                key="download_story_dual"
                            )
                            st.success("ì´ë¯¸ì§€ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
                        except Exception as e:
                            st.error(f"ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e}")

                with share_col2:
                    if st.button("ğŸ“¥ ì •ì‚¬ê°í˜• (1:1)", key="share_square_dual"):
                        try:
                            from components.share_image import create_mini_card_image

                            dim_scores_str = {
                                dim.value if hasattr(dim, 'value') else str(dim): score
                                for dim, score in worship_style.dimension_scores.items()
                            }

                            img_bytes = create_mini_card_image(
                                style_name=worship_style.style_name,
                                icon=worship_style.icon,
                                dimension_scores=dim_scores_str
                            )

                            st.download_button(
                                label="ğŸ’¾ ì´ë¯¸ì§€ ì €ì¥",
                                data=img_bytes,
                                file_name="worship_vocal_mini.png",
                                mime="image/png",
                                key="download_square_dual"
                            )
                            st.success("ì´ë¯¸ì§€ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
                        except Exception as e:
                            st.error(f"ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e}")

            # ğŸ“„ PDF ë¦¬í¬íŠ¸ ë‹¤ìš´ë¡œë“œ (ì´ì¤‘ ë¶„ì„)
            with st.expander("ğŸ“„ PDF ë¦¬í¬íŠ¸ ë‹¤ìš´ë¡œë“œ"):
                st.caption("ì´ì¤‘ ë¶„ì„ ê²°ê³¼ë¥¼ PDF íŒŒì¼ë¡œ ì €ì¥í•˜ì—¬ ë³´ê´€í•˜ê±°ë‚˜ ê³µìœ í•˜ì„¸ìš”!")

                if st.button("ğŸ“„ PDF ë¦¬í¬íŠ¸ ìƒì„±", key="generate_pdf_dual"):
                    try:
                        from components.pdf_report import generate_vocal_report_pdf

                        # ì°¨ì› ì ìˆ˜ ë³€í™˜
                        dim_scores_dict = {
                            dim.value if hasattr(dim, 'value') else str(dim): score
                            for dim, score in worship_style.dimension_scores.items()
                        }

                        # í‰ê·  features ê³„ì‚°
                        avg_features = {
                            'pitch_accuracy_cents': (features_a.get('pitch_accuracy_cents', 0) + features_b.get('pitch_accuracy_cents', 0)) / 2,
                            'high_note_stability': (features_a.get('high_note_stability', 0) + features_b.get('high_note_stability', 0)) / 2,
                            'dynamic_range_db': (features_a.get('dynamic_range_db', 0) + features_b.get('dynamic_range_db', 0)) / 2,
                            'pitch_mean': (features_a.get('pitch_mean', 0) + features_b.get('pitch_mean', 0)) / 2,
                            'avg_phrase_length': (features_a.get('avg_phrase_length', 0) + features_b.get('avg_phrase_length', 0)) / 2,
                            'vibrato_ratio': (features_a.get('vibrato_ratio', 0) + features_b.get('vibrato_ratio', 0)) / 2,
                            'rhythm_offset_ms': (features_a.get('rhythm_offset_ms', 0) + features_b.get('rhythm_offset_ms', 0)) / 2,
                        }

                        coaching_text = f"ì´ì¤‘ ë¶„ì„ ê²°ê³¼: {result.slow_song.song_title} + {result.fast_song.song_title}"

                        pdf_bytes = generate_vocal_report_pdf(
                            style_name=worship_style.style_name,
                            style_name_en=worship_style.style_name_en,
                            icon=worship_style.icon,
                            description=worship_style.description,
                            strengths=worship_style.strengths,
                            best_fit=worship_style.best_fit_contexts,
                            scorecard=dim_scores_dict,
                            features=avg_features,
                            coaching_text=coaching_text,
                            matching_songs=[],
                            challenge_songs=[]
                        )

                        if pdf_bytes[:4] == b'%PDF':
                            file_ext = "pdf"
                            mime_type = "application/pdf"
                        else:
                            file_ext = "txt"
                            mime_type = "text/plain"

                        st.download_button(
                            label=f"ğŸ’¾ ë¦¬í¬íŠ¸ ì €ì¥ (.{file_ext})",
                            data=pdf_bytes,
                            file_name=f"vocal_report_dual_{datetime.now().strftime('%Y%m%d')}.{file_ext}",
                            mime=mime_type,
                            key="download_pdf_dual"
                        )
                        st.success("ë¦¬í¬íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")

                    except Exception as e:
                        st.error(f"PDF ìƒì„± ì‹¤íŒ¨: {e}")
                        st.info("ğŸ’¡ PDF ìƒì„±ì„ ìœ„í•´ `pip install fpdf2` ì„¤ì¹˜ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

            st.markdown("---")

            # ë ˆì´ë” ì°¨íŠ¸ + DNA ì°¨íŠ¸
            col_chart1, col_chart2 = st.columns(2)

            with col_chart1:
                radar_fig = create_radar_chart(result.radar_stats, "ğŸ“Š VOCAL STAT RADAR")
                st.plotly_chart(radar_fig, use_container_width=True, key="dual_radar")

            with col_chart2:
                dna_fig = create_dna_chart(result.vocal_dna)
                st.plotly_chart(dna_fig, use_container_width=True, key="dual_dna")

            # ê³¡ë³„ ë¹„êµ ì°¨íŠ¸
            st.subheader("ğŸ“€ ê³¡ë³„ ë¹„êµ")

            comparison_fig = create_comparison_bar_chart(
                features_a, features_b,
                result.slow_song.song_title,
                result.fast_song.song_title
            )
            st.plotly_chart(comparison_fig, use_container_width=True, key="dual_comparison")

            # ìƒì„¸ ë¹„êµ í…Œì´ë¸”
            comp_col1, comp_col2 = st.columns(2)

            with comp_col1:
                st.markdown(f"**ğŸµ {result.slow_song.song_title}**")
                st.write(f"- í‰ê·  ìŒì—­: {features_a['avg_pitch_hz']:.1f} Hz ({hz_to_note_name(features_a['avg_pitch_hz'])})")
                st.write(f"- ìŒì—­í­: {features_a['pitch_range_semitones']:.1f} ë°˜ìŒ")
                st.write(f"- ë‹¤ì´ë‚˜ë¯¹ ë ˆì¸ì§€: {features_a['dynamic_range_db']:.1f} dB")
                st.write(f"- ìŒì • ì •í™•ë„: {features_a['pitch_accuracy_cents']:.1f} cents")
                st.write(f"- ê³ ìŒ ì•ˆì •ì„±: {features_a['high_note_stability']*100:.0f}%")

            with comp_col2:
                st.markdown(f"**ğŸµ {result.fast_song.song_title}**")
                st.write(f"- í‰ê·  ìŒì—­: {features_b['avg_pitch_hz']:.1f} Hz ({hz_to_note_name(features_b['avg_pitch_hz'])})")
                st.write(f"- ìŒì—­í­: {features_b['pitch_range_semitones']:.1f} ë°˜ìŒ")
                st.write(f"- ë‹¤ì´ë‚˜ë¯¹ ë ˆì¸ì§€: {features_b['dynamic_range_db']:.1f} dB")
                st.write(f"- ìŒì • ì •í™•ë„: {features_b['pitch_accuracy_cents']:.1f} cents")
                st.write(f"- ê³ ìŒ ì•ˆì •ì„±: {features_b['high_note_stability']*100:.0f}%")

            # ì²˜ë°©ì „ (LLM ê¸°ë°˜)
            if hasattr(result, 'solution') and result.solution:
                st.subheader("ğŸ’Š ì²˜ë°©ì „")
                st.warning(f"**ë¬¸ì œ**: {result.enemy_description}")
                st.success(f"**í•´ê²°ì±…**: {result.solution}")
                st.info(f"**ì˜¤ëŠ˜ì˜ ì—°ìŠµ**: {result.exercise}")

            # ì „ì²´ í‰ê°€ (LLM)
            if hasattr(result, 'overall_assessment') and result.overall_assessment:
                st.subheader("ğŸ’¬ AI ì½”ì¹˜ì˜ í•œë§ˆë””")
                st.success(result.overall_assessment)

        with tab2:
            # Song A ê¸°ìˆ ì  ë¶„ì„
            st.subheader(f"ğŸµ Song A: {result.slow_song.song_title}")
            render_technical_analysis(features_a, key_prefix="song_a")

        with tab3:
            # Song B ê¸°ìˆ ì  ë¶„ì„
            st.subheader(f"ğŸµ Song B: {result.fast_song.song_title}")
            render_technical_analysis(features_b, key_prefix="song_b")

        with tab4:
            # ì¶”ì²œ ì°¬ì–‘ íƒ­
            st.subheader("ğŸµ ì¶”ì²œ ì°¬ì–‘")
            st.markdown("AIê°€ ë‹¹ì‹ ì˜ ë³´ì»¬ ìŠ¤íƒ€ì¼ì„ ë¶„ì„í•˜ì—¬ ì¶”ì²œí•˜ëŠ” ì°¬ì–‘ì…ë‹ˆë‹¤.")

            # ì–´ìš¸ë¦¬ëŠ” ì°¬ì–‘
            st.markdown("### ğŸ’š ì–´ìš¸ë¦¬ëŠ” ì°¬ì–‘")
            st.info("í˜„ì¬ ë³´ì»¬ ìŠ¤íƒ€ì¼ê³¼ ì˜ ë§ëŠ” ê³¡ë“¤ì…ë‹ˆë‹¤. ê°•ì ì„ ì‚´ë ¤ ìì‹ ê° ìˆê²Œ ë¶ˆëŸ¬ë³´ì„¸ìš”!")

            if hasattr(result, 'matching_songs') and result.matching_songs:
                for i, song in enumerate(result.matching_songs, 1):
                    with st.container():
                        col1, col2 = st.columns([3, 1])
                        with col1:
                            st.markdown(f"**{i}. {song.title}** - {song.artist}")
                            st.write(f"ğŸ“ {song.reason}")
                        with col2:
                            if song.youtube_url:
                                st.link_button("â–¶ï¸ YouTube", song.youtube_url)
                        st.markdown("---")
            else:
                st.warning("ì¶”ì²œ ê³¡ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¶„ì„ì„ ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")

            st.markdown("---")

            # ë„ì „í•´ë³¼ ì°¬ì–‘
            st.markdown("### ğŸ”¥ ë„ì „í•´ë³¼ ì°¬ì–‘")
            st.warning("ì•½ì ì„ ê·¹ë³µí•˜ê³  ì„±ì¥í•˜ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” ê³¡ë“¤ì…ë‹ˆë‹¤. ì—°ìŠµìš©ìœ¼ë¡œ ë„ì „í•´ë³´ì„¸ìš”!")

            if hasattr(result, 'challenge_songs') and result.challenge_songs:
                for i, song in enumerate(result.challenge_songs, 1):
                    with st.container():
                        col1, col2 = st.columns([3, 1])
                        with col1:
                            st.markdown(f"**{i}. {song.title}** - {song.artist}")
                            st.write(f"ğŸ“ {song.reason}")
                        with col2:
                            if song.youtube_url:
                                st.link_button("â–¶ï¸ YouTube", song.youtube_url)
                        st.markdown("---")
            else:
                st.warning("ì¶”ì²œ ê³¡ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¶„ì„ì„ ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")

            # ì¶”ì²œ ê¸°ì¤€ ì„¤ëª…
            with st.expander("â„¹ï¸ ì¶”ì²œ ê¸°ì¤€"):
                st.markdown("""
                **ì–´ìš¸ë¦¬ëŠ” ì°¬ì–‘ ì„ ì • ê¸°ì¤€:**
                - í˜„ì¬ ìŒì—­ëŒ€ì— ë§ëŠ” ê³¡
                - ìŒìƒ‰ê³¼ ì–´ìš¸ë¦¬ëŠ” ì¥ë¥´/ë¶„ìœ„ê¸°
                - ê°•ì ì„ ì‚´ë¦´ ìˆ˜ ìˆëŠ” í…Œí¬ë‹‰ ìš”êµ¬ì‚¬í•­

                **ë„ì „ ì°¬ì–‘ ì„ ì • ê¸°ì¤€:**
                - ì•½ì  ì˜ì—­ì„ ì—°ìŠµí•  ìˆ˜ ìˆëŠ” ê³¡
                - ì ì ˆíˆ ë„ì „ì ì´ë©´ì„œ ë¶ˆê°€ëŠ¥í•˜ì§€ ì•Šì€ ë‚œì´ë„
                - ì„±ì¥ì— ë„ì›€ì´ ë˜ëŠ” íŠ¹ì • ê¸°ìˆ  ìš”êµ¬
                """)

        with tab5:
            # MBTI ì „ì²´ íƒ€ì… íƒ­
            st.subheader("ğŸ“‹ ë³´ì»¬ MBTI ì „ì²´ ìœ í˜•")
            st.markdown("6ê°€ì§€ ë³´ì»¬ MBTI ìœ í˜•ì„ í™•ì¸í•˜ê³ , ë‹¹ì‹ ì˜ íƒ€ì…ê³¼ ë¹„êµí•´ë³´ì„¸ìš”.")

            current_type = result.vocal_mbti
            st.info(f"ğŸ¯ **ë‹¹ì‹ ì˜ íƒ€ì…: {current_type}**")

            st.markdown("---")

            from vocal_mbti import VOCAL_TYPES
            for code, vtype in VOCAL_TYPES.items():
                is_current = code == current_type
                icon = "âœ… " if is_current else ""

                st.markdown(f"### {icon}{code}: {vtype.name_en}")
                st.markdown(f"**{vtype.name_kr}**")
                st.write(vtype.description)

                col1, col2 = st.columns(2)
                with col1:
                    st.markdown("**âœ¨ ê°•ì :**")
                    for s in vtype.strengths:
                        st.write(f"â€¢ {s}")
                with col2:
                    st.markdown("**ğŸ¤ ë¡¤ëª¨ë¸:**")
                    for r in vtype.role_models:
                        st.write(f"â€¢ {r}")

                st.markdown("---")

        with tab6:
            # ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ íƒ­ (ì´ì¤‘ ë¶„ì„ìš©)
            st.subheader("ğŸ“¥ ë¶„ë¦¬ëœ ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ")

            if hasattr(st.session_state, 'dual_separation_result') and st.session_state.dual_separation_result:
                sep_data = st.session_state.dual_separation_result

                # Song A ë‹¤ìš´ë¡œë“œ
                st.markdown(f"### ğŸµ {sep_data['song_a']['title']}")
                st.success(f"âœ… ë³´ì»¬ ë¶„ë¦¬ ì™„ë£Œ! (ì‹ ë¢°ë„: {sep_data['song_a']['confidence'] * 100:.0f}%)")

                col_a1, col_a2 = st.columns(2)

                with col_a1:
                    st.markdown("**ğŸ¤ ë³´ì»¬ íŠ¸ë™**")
                    if sep_data['song_a']['vocals_path'] and os.path.exists(sep_data['song_a']['vocals_path']):
                        with open(sep_data['song_a']['vocals_path'], 'rb') as f:
                            vocals_data_a = f.read()
                        st.audio(vocals_data_a, format='audio/wav')
                        st.download_button(
                            label="ğŸ“¥ ë³´ì»¬ ë‹¤ìš´ë¡œë“œ (WAV)",
                            data=vocals_data_a,
                            file_name=f"vocals_{sep_data['song_a']['title']}.wav",
                            mime="audio/wav",
                            key="download_vocals_a"
                        )
                    else:
                        st.warning("ë³´ì»¬ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

                with col_a2:
                    st.markdown("**ğŸ¹ ë°˜ì£¼ íŠ¸ë™**")
                    if sep_data['song_a']['instrumental_path'] and os.path.exists(sep_data['song_a']['instrumental_path']):
                        with open(sep_data['song_a']['instrumental_path'], 'rb') as f:
                            instrumental_data_a = f.read()
                        st.audio(instrumental_data_a, format='audio/wav')
                        st.download_button(
                            label="ğŸ“¥ ë°˜ì£¼ ë‹¤ìš´ë¡œë“œ (WAV)",
                            data=instrumental_data_a,
                            file_name=f"instrumental_{sep_data['song_a']['title']}.wav",
                            mime="audio/wav",
                            key="download_instrumental_a"
                        )
                    else:
                        st.warning("ë°˜ì£¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

                st.markdown("---")

                # Song B ë‹¤ìš´ë¡œë“œ
                st.markdown(f"### ğŸµ {sep_data['song_b']['title']}")
                st.success(f"âœ… ë³´ì»¬ ë¶„ë¦¬ ì™„ë£Œ! (ì‹ ë¢°ë„: {sep_data['song_b']['confidence'] * 100:.0f}%)")

                col_b1, col_b2 = st.columns(2)

                with col_b1:
                    st.markdown("**ğŸ¤ ë³´ì»¬ íŠ¸ë™**")
                    if sep_data['song_b']['vocals_path'] and os.path.exists(sep_data['song_b']['vocals_path']):
                        with open(sep_data['song_b']['vocals_path'], 'rb') as f:
                            vocals_data_b = f.read()
                        st.audio(vocals_data_b, format='audio/wav')
                        st.download_button(
                            label="ğŸ“¥ ë³´ì»¬ ë‹¤ìš´ë¡œë“œ (WAV)",
                            data=vocals_data_b,
                            file_name=f"vocals_{sep_data['song_b']['title']}.wav",
                            mime="audio/wav",
                            key="download_vocals_b"
                        )
                    else:
                        st.warning("ë³´ì»¬ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

                with col_b2:
                    st.markdown("**ğŸ¹ ë°˜ì£¼ íŠ¸ë™**")
                    if sep_data['song_b']['instrumental_path'] and os.path.exists(sep_data['song_b']['instrumental_path']):
                        with open(sep_data['song_b']['instrumental_path'], 'rb') as f:
                            instrumental_data_b = f.read()
                        st.audio(instrumental_data_b, format='audio/wav')
                        st.download_button(
                            label="ğŸ“¥ ë°˜ì£¼ ë‹¤ìš´ë¡œë“œ (WAV)",
                            data=instrumental_data_b,
                            file_name=f"instrumental_{sep_data['song_b']['title']}.wav",
                            mime="audio/wav",
                            key="download_instrumental_b"
                        )
                    else:
                        st.warning("ë°˜ì£¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

                st.markdown("---")
                st.info("ğŸ’¡ **í™œìš© íŒ:** ë¶„ë¦¬ëœ ë³´ì»¬ë¡œ ìŒì • ì—°ìŠµì„, ë°˜ì£¼ë¡œ MR ì—°ìŠµì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")

            else:
                st.info("ğŸ¤ ë³´ì»¬ ë¶„ë¦¬ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                st.write("'ë°˜ì£¼ì™€ í•¨ê»˜' ë˜ëŠ” 'ì°¬ì–‘íŒ€ê³¼ í•¨ê»˜' ì˜µì…˜ìœ¼ë¡œ ë¶„ì„í•˜ë©´ ë¶„ë¦¬ëœ ì˜¤ë””ì˜¤ë¥¼ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")


# =============================================
# ë‹¨ì¼ ë¶„ì„ ëª¨ë“œ (ê¸°ì¡´ ë¡œì§)
# =============================================

else:
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if 'analysis_result' not in st.session_state:
        st.session_state.analysis_result = None
    if 'separation_result' not in st.session_state:
        st.session_state.separation_result = None

    st.header("1ï¸âƒ£ ì°¬ì–‘ ì—…ë¡œë“œ")

    input_method = st.radio(
        "ì…ë ¥ ë°©ì‹ì„ ì„ íƒí•˜ì„¸ìš”",
        ["ğŸ”— YouTube ë§í¬", "ğŸ“ íŒŒì¼ ì—…ë¡œë“œ"],
        horizontal=True
    )

    audio_path = None

    if input_method == "ğŸ”— YouTube ë§í¬":
        url = st.text_input("YouTube URLì„ ì…ë ¥í•˜ì„¸ìš”", placeholder="https://www.youtube.com/watch?v=...")

        # êµ¬ê°„ ì„ íƒ UI ê°œì„ 
        st.markdown("##### âœ‚ï¸ êµ¬ê°„ ì„ íƒ")
        st.caption("ë¶„ì„í•  êµ¬ê°„ì„ ì§€ì •í•˜ì„¸ìš”. ë¹„ì›Œë‘ë©´ ì „ì²´ ì˜ìƒì„ ë¶„ì„í•©ë‹ˆë‹¤.")

        col1, col2, col3 = st.columns([2, 2, 1])
        with col1:
            start_time = st.text_input("â±ï¸ ì‹œì‘", "0:00", help="í˜•ì‹: MM:SS ë˜ëŠ” HH:MM:SS")
        with col2:
            end_time = st.text_input("â±ï¸ ì¢…ë£Œ", "", help="ë¹„ì›Œë‘ë©´ ëê¹Œì§€ ì¶”ì¶œ")
        with col3:
            # ì˜ˆìƒ ê¸¸ì´ í‘œì‹œ
            if start_time and end_time:
                try:
                    start_sec = time_to_seconds(start_time) or 0
                    end_sec = time_to_seconds(end_time)
                    if end_sec and end_sec > start_sec:
                        duration = end_sec - start_sec
                        st.metric("ê¸¸ì´", f"{duration//60}:{duration%60:02d}")
                except:
                    pass

        # ë¹ ë¥¸ êµ¬ê°„ ì„ íƒ ë²„íŠ¼
        st.caption("ğŸ’¡ ë¹ ë¥¸ ì„ íƒ:")
        qcol1, qcol2, qcol3, qcol4 = st.columns(4)
        with qcol1:
            if st.button("1ë¶„", key="q1"):
                st.session_state.quick_duration = 60
        with qcol2:
            if st.button("2ë¶„", key="q2"):
                st.session_state.quick_duration = 120
        with qcol3:
            if st.button("3ë¶„", key="q3"):
                st.session_state.quick_duration = 180
        with qcol4:
            if st.button("ì „ì²´", key="qall"):
                st.session_state.quick_duration = None

        if st.button("ğŸµ ì˜¤ë””ì˜¤ ì¶”ì¶œ", type="primary") and url:
            with st.spinner("YouTubeì—ì„œ ì˜¤ë””ì˜¤ ì¶”ì¶œ ì¤‘..."):
                try:
                    audio_path, video_title = extract_youtube_audio(url, start_time, end_time, "single_analysis")
                    st.session_state.single_audio_path = audio_path
                    st.session_state.single_video_title = video_title
                    st.success(f"âœ… ì¶”ì¶œ ì™„ë£Œ! ({video_title})")
                    st.audio(audio_path)
                except Exception as e:
                    st.error(f"ì¶”ì¶œ ì‹¤íŒ¨: {e}")

    elif input_method == "ğŸ“ íŒŒì¼ ì—…ë¡œë“œ":
        uploaded_file = st.file_uploader(
            "ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”",
            type=['mp3', 'wav', 'm4a', 'ogg']
        )

        if uploaded_file:
            temp_path = f"/tmp/{uploaded_file.name}"
            with open(temp_path, "wb") as f:
                f.write(uploaded_file.getbuffer())
            audio_path = temp_path
            st.session_state.single_audio_path = audio_path
            # P1: íŒŒì¼ëª… ì €ì¥ (íˆìŠ¤í† ë¦¬ìš©)
            st.session_state.uploaded_file_name = uploaded_file.name.rsplit('.', 1)[0]
            st.audio(uploaded_file)

    # ì´ì „ì— ì¶”ì¶œí•œ ì˜¤ë””ì˜¤ ì‚¬ìš©
    if 'single_audio_path' in st.session_state:
        audio_path = st.session_state.single_audio_path

    # ë…¹ìŒ í™˜ê²½ ì„¤ì •
    if audio_path:
        st.header("2ï¸âƒ£ ë…¹ìŒ í™˜ê²½")

        recording_type = st.selectbox(
            "ë…¹ìŒ ìƒí™©ì„ ì„ íƒí•˜ì„¸ìš”",
            [
                "ğŸ¤ ì†”ë¡œ ë…¹ìŒ (ë‚˜ë§Œ ë…¹ìŒë¨)",
                "ğŸ¹ ë°˜ì£¼ì™€ í•¨ê»˜ (MR + ë‚´ ëª©ì†Œë¦¬)",
                "ğŸ‘¥ ì°¬ì–‘íŒ€ê³¼ í•¨ê»˜ (ë‚´ê°€ ë©”ì¸ ì¸ë„ì)",
                "ğŸµ ì—¬ëŸ¬ ì‹±ì–´ ì¤‘ í•˜ë‚˜ (í•˜ëª¨ë‹ˆ íŒŒíŠ¸)"
            ],
            help="ë…¹ìŒ í™˜ê²½ì— ë”°ë¼ AI ë³´ì»¬ ë¶„ë¦¬ ì—¬ë¶€ê°€ ê²°ì •ë©ë‹ˆë‹¤. ì •í™•í•œ ë¶„ì„ì„ ìœ„í•´ ì‹¤ì œ ë…¹ìŒ ìƒí™©ì„ ì„ íƒí•´ì£¼ì„¸ìš”."
        )

        # P1: ì˜µì…˜ë³„ ìƒì„¸ ì„¤ëª…
        with st.expander("â„¹ï¸ ë…¹ìŒ í™˜ê²½ ì„¤ëª…", expanded=False):
            st.markdown("""
| ì˜µì…˜ | ì„¤ëª… | AI ë³´ì»¬ ë¶„ë¦¬ |
|------|------|-------------|
| **ğŸ¤ ì†”ë¡œ ë…¹ìŒ** | ëª©ì†Œë¦¬ë§Œ ë…¹ìŒëœ íŒŒì¼ (ì•„ì¹´í ë¼, ë³´ì´ìŠ¤ë©”ëª¨ ë“±) | âŒ ë¶ˆí•„ìš” |
| **ğŸ¹ ë°˜ì£¼ì™€ í•¨ê»˜** | MR + ë‚´ ëª©ì†Œë¦¬ê°€ í•¨ê»˜ ë…¹ìŒë¨ | âœ… ìë™ ë¶„ë¦¬ |
| **ğŸ‘¥ ì°¬ì–‘íŒ€ê³¼ í•¨ê»˜** | ë‚´ê°€ ë©”ì¸ì´ê³  ë°°ê²½ì— ë‹¤ë¥¸ ì‹±ì–´ê°€ ìˆìŒ | âœ… ìë™ ë¶„ë¦¬ |
| **ğŸµ í•˜ëª¨ë‹ˆ íŒŒíŠ¸** | ì—¬ëŸ¬ ëª…ì´ í•¨ê»˜ ë¶€ë¥´ê³  ë‚´ê°€ ê·¸ ì¤‘ í•˜ë‚˜ | âš ï¸ ì •í™•ë„ ë‚®ìŒ |
            """)

        need_separation = recording_type != "ğŸ¤ ì†”ë¡œ ë…¹ìŒ (ë‚˜ë§Œ ë…¹ìŒë¨)"

        if need_separation:
            st.info("ğŸ”§ AIê°€ ìë™ìœ¼ë¡œ ë³´ì»¬ì„ ë¶„ë¦¬í•˜ì—¬ ë¶„ì„í•©ë‹ˆë‹¤. íŒŒì¼ ê¸¸ì´ì— ë”°ë¼ 1-5ë¶„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            if recording_type == "ğŸµ ì—¬ëŸ¬ ì‹±ì–´ ì¤‘ í•˜ë‚˜ (í•˜ëª¨ë‹ˆ íŒŒíŠ¸)":
                st.warning("âš ï¸ í•˜ëª¨ë‹ˆ ì¤‘ íŠ¹ì • íŒŒíŠ¸ ë¶„ë¦¬ëŠ” ì •í™•ë„ê°€ ë‚®ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê°€ëŠ¥í•˜ë©´ ì†”ë¡œ ë…¹ìŒì„ ê¶Œì¥í•©ë‹ˆë‹¤.")

    # ë¶„ì„ ì‹¤í–‰
    if audio_path and st.button("ğŸ” AI ë¶„ì„ ì‹œì‘", type="primary"):

        # P0: ë¡œë”© ë‹¨ê³„ ìƒì„¸ í‘œì‹œ
        progress = st.progress(0)
        status_container = st.container()
        with status_container:
            status = st.empty()
            detail = st.empty()

        try:
            # ë³´ì»¬ ë¶„ë¦¬ (í•„ìš”ì‹œ)
            if need_separation:
                status.markdown("### ğŸ­ ë³´ì»¬ ë¶„ë¦¬ ì¤‘...")
                detail.caption("AIê°€ ëª©ì†Œë¦¬ë§Œ ì¶”ì¶œí•˜ê³  ìˆì–´ìš”. íŒŒì¼ ê¸¸ì´ì— ë”°ë¼ 1-5ë¶„ ì •ë„ ì†Œìš”ë©ë‹ˆë‹¤.")
                from vocal_separator import auto_separate, SeparationMode

                sep_result = auto_separate(
                    audio_path,
                    "/tmp/separated",
                    mode=SeparationMode.VOCALS_ONLY
                )

                if sep_result.success and sep_result.lead_vocals_path:
                    audio_path = sep_result.lead_vocals_path
                    st.info(f"âœ… ë³´ì»¬ ë¶„ë¦¬ ì™„ë£Œ! (ì‹ ë¢°ë„: {sep_result.confidence * 100:.0f}%)")
                    # ë¶„ë¦¬ ê²°ê³¼ ì €ì¥ (ë‹¤ìš´ë¡œë“œìš©)
                    st.session_state.separation_result = {
                        'vocals_path': sep_result.lead_vocals_path,
                        'instrumental_path': sep_result.instrumental_path,
                        'confidence': sep_result.confidence
                    }
                progress.progress(25)

            # íŠ¹ì§• ì¶”ì¶œ (ì‹œê³„ì—´ ë°ì´í„° í¬í•¨)
            status.markdown("### ğŸ“Š ìŒì„± ë¶„ì„ ì¤‘...")
            detail.caption("í”¼ì¹˜, ìŒëŸ‰, ìŒìƒ‰ì„ ë¶„ì„í•˜ê³  ìˆì–´ìš”.")
            features = analyze_audio_features(audio_path, include_timeseries=True)
            progress.progress(50)

            # MBTI ë¶„ë¥˜
            status.markdown("### ğŸ§¬ ë³´ì»¬ DNA ê³„ì‚° ì¤‘...")
            detail.caption("ë‹¹ì‹ ì˜ ë³´ì»¬ ìŠ¤íƒ€ì¼ì„ íŒŒì•…í•˜ê³  ìˆì–´ìš”.")

            from vocal_mbti import VocalFeatures, classify_vocal_type, VOCAL_TYPES, calculate_scorecard

            vocal_features = VocalFeatures(
                pitch_range_semitones=features['pitch_range_semitones'],
                avg_pitch_hz=features['avg_pitch_hz'],
                high_note_ratio=features['high_note_ratio'],
                low_note_ratio=features['low_note_ratio'],
                dynamic_range_db=features['dynamic_range_db'],
                energy_variance=features['energy_variance'],
                climax_intensity=features['climax_intensity'],
                spectral_centroid_hz=features['spectral_centroid_hz'],
                warmth_score=features['warmth_score'],
                vibrato_ratio=features['vibrato_ratio'],
                pitch_stability=features['pitch_stability'],
                pitch_accuracy_cents=features['pitch_accuracy_cents'],
                tempo_bpm=features['tempo_bpm'],
                breath_phrase_length=features['breath_phrase_length'],
                flat_tendency=features['flat_tendency'],
                sharp_tendency=features['sharp_tendency']
            )

            primary_type, scores = classify_vocal_type(vocal_features)
            vocal_type_info = VOCAL_TYPES[primary_type]
            scorecard = calculate_scorecard(vocal_features)
            progress.progress(75)

            # ê°ì„± í•´ì„
            status.markdown("### ğŸ’ í”¼ë“œë°± ìƒì„± ì¤‘...")
            detail.caption("ë§ì¶¤ í”¼ë“œë°±ì„ ì‘ì„±í•˜ê³  ìˆì–´ìš”.")
            from emotional_interpreter import generate_local_feedback
            feedback = generate_local_feedback(vocal_features, vocal_type_info, scorecard)
            progress.progress(85)

            # LLM ê¸°ë°˜ ì¶”ì²œê³¡ ë¶„ì„
            status.markdown("### ğŸ¤– AI ì½”ì¹­ ìƒì„± ì¤‘...")
            detail.caption("Claude AIê°€ ë§ì¶¤ ì½”ì¹­ê³¼ ì¶”ì²œê³¡ì„ ì¤€ë¹„í•˜ê³  ìˆì–´ìš”.")
            from llm_analyzer import analyze_single_with_llm
            llm_single_result = analyze_single_with_llm(features, "ë¶„ì„ëœ ê³¡")
            progress.progress(100)

            status.markdown("### âœ… ë¶„ì„ ì™„ë£Œ!")
            detail.caption("ê²°ê³¼ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”!")

            # ê²°ê³¼ ì €ì¥
            st.session_state.analysis_result = {
                'features': vocal_features,
                'raw_features': features,
                'primary_type': primary_type,
                'scores': scores,
                'vocal_type_info': vocal_type_info,
                'scorecard': scorecard,
                'feedback': feedback,
                'llm_result': llm_single_result
            }

            # P1: íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
            song_title = st.session_state.get('single_video_title', st.session_state.get('uploaded_file_name', 'ë¶„ì„ëœ ê³¡'))
            st.session_state.analysis_history.append({
                'timestamp': datetime.now(),
                'song_title': song_title,
                'mbti_type': primary_type,
            })

        except Exception as e:
            st.error(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            st.code(traceback.format_exc())

    # ê²°ê³¼ í‘œì‹œ
    if st.session_state.analysis_result:
        result = st.session_state.analysis_result

        st.header("3ï¸âƒ£ ë¶„ì„ ê²°ê³¼")

        # íƒ­ ì¸í„°í˜ì´ìŠ¤
        tab1, tab2, tab3, tab4, tab5 = st.tabs(["ğŸ­ ë³´ì»¬ ì½”ì¹­", "ğŸ“Š ê¸°ìˆ ì  ë¶„ì„", "ğŸµ ì¶”ì²œ ì°¬ì–‘", "ğŸ“‹ ë³´ì»¬ MBTI ìœ í˜•", "ğŸ“¥ ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ"])

        with tab1:
            # ğŸ§¬ VOCAL IDENTITY ì„¹ì…˜ (ì´ì¤‘ë¶„ì„ ìŠ¤íƒ€ì¼)
            st.markdown("### ğŸ§¬ VOCAL IDENTITY")

            id_col1, id_col2 = st.columns([1, 2])

            with id_col1:
                st.metric("MBTI íƒ€ì…", result['primary_type'])
                st.markdown(f"**{result['vocal_type_info'].name_en}**")

            with id_col2:
                st.markdown(f"**{result['vocal_type_info'].name_kr}**")
                st.write(result['vocal_type_info'].description)
                st.markdown("**ğŸ¤ ë¡¤ëª¨ë¸:** " + ", ".join(result['vocal_type_info'].role_models))

            st.markdown("---")

            # â­ YOUR SIGNATURE / ğŸ¯ GROWTH POINT (2ì—´)
            col1, col2 = st.columns(2)

            with col1:
                st.markdown("### â­ YOUR SIGNATURE")
                st.success("**ë‹¹ì‹ ì˜ ê°•ì **")
                for s in result['vocal_type_info'].strengths:
                    st.write(f"âœ… {s}")

            with col2:
                st.markdown("### ğŸ¯ GROWTH POINT")
                st.warning("**ì„±ì¥ í¬ì¸íŠ¸**")
                for w in result['vocal_type_info'].weaknesses:
                    st.write(f"ğŸ“Œ {w}")

            st.markdown("---")

            # â›ª ì°¬ì–‘ ì˜ˆë°° ìŠ¤íƒ€ì¼
            st.markdown("### â›ª ì°¬ì–‘ ì˜ˆë°° ìŠ¤íƒ€ì¼")
            from worship_style import calculate_worship_style, WORSHIP_STYLE_AXES, StyleDimension

            features = result['raw_features']
            worship_style = calculate_worship_style(features)

            st.success(f"{worship_style.icon} **{worship_style.style_name}** ({worship_style.style_name_en})")
            st.write(worship_style.description)

            style_col1, style_col2 = st.columns(2)
            with style_col1:
                st.markdown("**âœ¨ ê°•ì :**")
                for strength in worship_style.strengths:
                    st.write(f"â€¢ {strength}")
            with style_col2:
                st.markdown("**â›ª ì–´ìš¸ë¦¬ëŠ” ì˜ˆë°°:**")
                for context in worship_style.best_fit_contexts:
                    st.write(f"â€¢ {context}")

            with st.expander("ğŸ“Š ìŠ¤íƒ€ì¼ ìƒì„¸ ë¶„ì„"):
                for dim, score in worship_style.dimension_scores.items():
                    axis = WORSHIP_STYLE_AXES[dim]
                    st.write(f"**{axis.low_icon} {axis.low_label}** â† â†’ **{axis.high_label} {axis.high_icon}**")
                    st.progress(float(score))
                    if score < 0.35:
                        st.caption(f"â†’ {axis.worship_context_low}")
                    elif score > 0.65:
                        st.caption(f"â†’ {axis.worship_context_high}")
                    else:
                        st.caption("â†’ ë‹¤ì–‘í•œ ìƒí™©ì— ìœ ì—°í•˜ê²Œ ì ì‘")

            # ğŸ“± SNS ê³µìœ  ì´ë¯¸ì§€ ìƒì„±
            with st.expander("ğŸ“± SNS ê³µìœ  ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ"):
                st.caption("í˜ë¥´ì†Œë‚˜ ì¹´ë“œë¥¼ ì´ë¯¸ì§€ë¡œ ì €ì¥í•˜ì—¬ SNSì— ê³µìœ í•˜ì„¸ìš”!")
                share_col1, share_col2 = st.columns(2)

                with share_col1:
                    if st.button("ğŸ“¥ ìŠ¤í† ë¦¬ìš© (9:16)", key="share_story_single"):
                        try:
                            from components.share_image import create_persona_card_image

                            # dimension_scoresë¥¼ ë¬¸ìì—´ í‚¤ë¡œ ë³€í™˜
                            dim_scores_str = {
                                dim.value if hasattr(dim, 'value') else str(dim): score
                                for dim, score in worship_style.dimension_scores.items()
                            }

                            img_bytes = create_persona_card_image(
                                style_name=worship_style.style_name,
                                style_name_en=worship_style.style_name_en,
                                icon=worship_style.icon,
                                description=worship_style.description,
                                strengths=worship_style.strengths,
                                best_fit_contexts=worship_style.best_fit_contexts,
                                dimension_scores=dim_scores_str
                            )

                            st.download_button(
                                label="ğŸ’¾ ì´ë¯¸ì§€ ì €ì¥",
                                data=img_bytes,
                                file_name="worship_vocal_persona.png",
                                mime="image/png",
                                key="download_story_single"
                            )
                            st.success("ì´ë¯¸ì§€ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
                        except Exception as e:
                            st.error(f"ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e}")

                with share_col2:
                    if st.button("ğŸ“¥ ì •ì‚¬ê°í˜• (1:1)", key="share_square_single"):
                        try:
                            from components.share_image import create_mini_card_image

                            dim_scores_str = {
                                dim.value if hasattr(dim, 'value') else str(dim): score
                                for dim, score in worship_style.dimension_scores.items()
                            }

                            img_bytes = create_mini_card_image(
                                style_name=worship_style.style_name,
                                icon=worship_style.icon,
                                dimension_scores=dim_scores_str
                            )

                            st.download_button(
                                label="ğŸ’¾ ì´ë¯¸ì§€ ì €ì¥",
                                data=img_bytes,
                                file_name="worship_vocal_mini.png",
                                mime="image/png",
                                key="download_square_single"
                            )
                            st.success("ì´ë¯¸ì§€ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
                        except Exception as e:
                            st.error(f"ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e}")

            # ğŸ“„ PDF ë¦¬í¬íŠ¸ ë‹¤ìš´ë¡œë“œ
            with st.expander("ğŸ“„ PDF ë¦¬í¬íŠ¸ ë‹¤ìš´ë¡œë“œ"):
                st.caption("ë¶„ì„ ê²°ê³¼ë¥¼ PDF íŒŒì¼ë¡œ ì €ì¥í•˜ì—¬ ë³´ê´€í•˜ê±°ë‚˜ ê³µìœ í•˜ì„¸ìš”!")

                if st.button("ğŸ“„ PDF ë¦¬í¬íŠ¸ ìƒì„±", key="generate_pdf_single"):
                    try:
                        from components.pdf_report import generate_vocal_report_pdf

                        # ì°¨ì› ì ìˆ˜ ë³€í™˜
                        dim_scores_dict = {
                            dim.value if hasattr(dim, 'value') else str(dim): score
                            for dim, score in worship_style.dimension_scores.items()
                        }

                        # LLM ê²°ê³¼ì—ì„œ ì¶”ì²œ ê³¡ ì¶”ì¶œ
                        llm_result = result.get('llm_result')
                        matching = []
                        challenge = []
                        coaching_text = ""

                        if llm_result:
                            if hasattr(llm_result, 'matching_songs'):
                                matching = [s.name if hasattr(s, 'name') else str(s) for s in llm_result.matching_songs[:5]]
                            if hasattr(llm_result, 'challenge_songs'):
                                challenge = [s.name if hasattr(s, 'name') else str(s) for s in llm_result.challenge_songs[:5]]
                            if hasattr(llm_result, 'coaching_summary'):
                                coaching_text = llm_result.coaching_summary

                        pdf_bytes = generate_vocal_report_pdf(
                            style_name=worship_style.style_name,
                            style_name_en=worship_style.style_name_en,
                            icon=worship_style.icon,
                            description=worship_style.description,
                            strengths=worship_style.strengths,
                            best_fit=worship_style.best_fit_contexts,
                            scorecard=dim_scores_dict,
                            features=result['raw_features'],
                            coaching_text=coaching_text,
                            matching_songs=matching,
                            challenge_songs=challenge
                        )

                        # PDFì¸ì§€ í…ìŠ¤íŠ¸ì¸ì§€ í™•ì¸
                        if pdf_bytes[:4] == b'%PDF':
                            file_ext = "pdf"
                            mime_type = "application/pdf"
                        else:
                            file_ext = "txt"
                            mime_type = "text/plain"

                        st.download_button(
                            label=f"ğŸ’¾ ë¦¬í¬íŠ¸ ì €ì¥ (.{file_ext})",
                            data=pdf_bytes,
                            file_name=f"vocal_report_{datetime.now().strftime('%Y%m%d')}.{file_ext}",
                            mime=mime_type,
                            key="download_pdf_single"
                        )
                        st.success("ë¦¬í¬íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")

                    except Exception as e:
                        st.error(f"PDF ìƒì„± ì‹¤íŒ¨: {e}")
                        st.info("ğŸ’¡ PDF ìƒì„±ì„ ìœ„í•´ `pip install fpdf2` ì„¤ì¹˜ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

            st.markdown("---")

            # ğŸ“Š ë ˆì´ë” ì°¨íŠ¸ + ìŠ¤ì½”ì–´ì¹´ë“œ (2ì—´)
            chart_col1, chart_col2 = st.columns(2)

            with chart_col1:
                # ë ˆì´ë” ì°¨íŠ¸ ìƒì„± (featuresì—ì„œ)
                features = result['raw_features']
                radar_stats = {
                    "ìŒì •": max(0, min(100, 100 - features['pitch_accuracy_cents'] * 2)),
                    "ê³ ìŒ": features.get('high_note_stability', 0.8) * 100,
                    "í˜¸í¡": min(100, features.get('breath_phrase_length', 3) * 15),
                    "ë‹¤ì´ë‚˜ë¯¹": min(100, features['dynamic_range_db'] * 5),
                    "ì•ˆì •ì„±": features.get('pitch_stability', 0.7) * 100
                }
                radar_fig = create_radar_chart(radar_stats, "ğŸ“Š VOCAL STAT RADAR")
                st.plotly_chart(radar_fig, use_container_width=True, key="single_radar")

            with chart_col2:
                # ìŠ¤ì½”ì–´ì¹´ë“œ (ì‹œê°ì ìœ¼ë¡œ ê°œì„ )
                st.markdown("### ğŸ“‹ ì—­ëŸ‰ ìŠ¤ì½”ì–´ì¹´ë“œ")
                sc = result['scorecard']

                score_items = [
                    ("ğŸµ ìŒìƒ‰+ì•ˆì •", sc.tone),
                    ("ğŸ‘‘ ë¦¬ë”©", sc.leadership),
                    ("ğŸ¥ ë¦¬ë“¬", sc.rhythm),
                    ("ğŸ’¬ ì „ë‹¬ë ¥", sc.diction),
                    ("ğŸ”§ í…Œí¬ë‹‰", sc.technique)
                ]

                for label, score in score_items:
                    emoji = "ğŸŸ¢" if score >= 4 else "ğŸŸ¡" if score >= 3 else "ğŸ”´"
                    st.markdown(f"{emoji} **{label}**: {score}/5")

                st.metric("ğŸ“Š ì¢…í•©", f"{sc.total}/100")

            st.markdown("---")

            # ğŸ’Š ì²˜ë°©ì „ ìŠ¤íƒ€ì¼ í”¼ë“œë°±
            st.markdown("### ğŸ’Š AI ì½”ì¹­ ì²˜ë°©ì „")

            # í”¼ë“œë°± ìš”ì•½
            st.success(f"**ğŸ’¬ AI ì½”ì¹˜ì˜ í•œë§ˆë””**\n\n{result['feedback'].summary}")

            with st.expander("ğŸ“ ìƒì„¸ ë¶„ì„ ë³´ê¸°"):
                st.markdown(result['feedback'].detailed_feedback)

            st.markdown("---")

            # ğŸ¯ ì˜¤ëŠ˜ì˜ ì—°ìŠµ (ì‹œê°ì ìœ¼ë¡œ ê°œì„ )
            st.markdown("### ğŸ¯ ì˜¤ëŠ˜ì˜ 5ë¶„ ì—°ìŠµ")

            for i, ex in enumerate(result['feedback'].exercises, 1):
                with st.container():
                    st.info(f"**{i}. {ex['name']}** ({ex['duration']})\n\n{ex['description']}")

            st.markdown("---")

            # ğŸ“Š íƒ€ì…ë³„ ë§¤ì¹­ ì ìˆ˜ (ì ‘íˆëŠ” ì„¹ì…˜ìœ¼ë¡œ)
            with st.expander("ğŸ“Š íƒ€ì…ë³„ ë§¤ì¹­ ì ìˆ˜ ë³´ê¸°"):
                import pandas as pd
                from vocal_mbti import VOCAL_TYPES

                score_df = pd.DataFrame([
                    {"íƒ€ì…": VOCAL_TYPES[code].name_kr, "ì ìˆ˜": score}
                    for code, score in sorted(result['scores'].items(), key=lambda x: x[1], reverse=True)
                ])
                st.bar_chart(score_df.set_index("íƒ€ì…"))

        with tab2:
            # ê¸°ìˆ ì  ë¶„ì„ íƒ­
            render_technical_analysis(result['raw_features'], result['scorecard'])

        with tab3:
            # ì¶”ì²œ ì°¬ì–‘ íƒ­
            st.subheader("ğŸµ ì¶”ì²œ ì°¬ì–‘")
            st.markdown("AIê°€ ë‹¹ì‹ ì˜ ë³´ì»¬ ìŠ¤íƒ€ì¼ì„ ë¶„ì„í•˜ì—¬ ì¶”ì²œí•˜ëŠ” ì°¬ì–‘ì…ë‹ˆë‹¤.")

            llm_result = result.get('llm_result')

            # ì–´ìš¸ë¦¬ëŠ” ì°¬ì–‘
            st.markdown("### ğŸ’š ì–´ìš¸ë¦¬ëŠ” ì°¬ì–‘")
            st.info("í˜„ì¬ ë³´ì»¬ ìŠ¤íƒ€ì¼ê³¼ ì˜ ë§ëŠ” ê³¡ë“¤ì…ë‹ˆë‹¤. ê°•ì ì„ ì‚´ë ¤ ìì‹ ê° ìˆê²Œ ë¶ˆëŸ¬ë³´ì„¸ìš”!")

            if llm_result and hasattr(llm_result, 'matching_songs') and llm_result.matching_songs:
                for i, song in enumerate(llm_result.matching_songs, 1):
                    with st.container():
                        col1, col2 = st.columns([3, 1])
                        with col1:
                            st.markdown(f"**{i}. {song.title}** - {song.artist}")
                            st.write(f"ğŸ“ {song.reason}")
                        with col2:
                            if song.youtube_url:
                                st.link_button("â–¶ï¸ YouTube", song.youtube_url)
                        st.markdown("---")
            else:
                st.warning("ì¶”ì²œ ê³¡ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¶„ì„ì„ ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")

            st.markdown("---")

            # ë„ì „í•´ë³¼ ì°¬ì–‘
            st.markdown("### ğŸ”¥ ë„ì „í•´ë³¼ ì°¬ì–‘")
            st.warning("ì•½ì ì„ ê·¹ë³µí•˜ê³  ì„±ì¥í•˜ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” ê³¡ë“¤ì…ë‹ˆë‹¤. ì—°ìŠµìš©ìœ¼ë¡œ ë„ì „í•´ë³´ì„¸ìš”!")

            if llm_result and hasattr(llm_result, 'challenge_songs') and llm_result.challenge_songs:
                for i, song in enumerate(llm_result.challenge_songs, 1):
                    with st.container():
                        col1, col2 = st.columns([3, 1])
                        with col1:
                            st.markdown(f"**{i}. {song.title}** - {song.artist}")
                            st.write(f"ğŸ“ {song.reason}")
                        with col2:
                            if song.youtube_url:
                                st.link_button("â–¶ï¸ YouTube", song.youtube_url)
                        st.markdown("---")
            else:
                st.warning("ì¶”ì²œ ê³¡ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¶„ì„ì„ ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")

            # ì¶”ì²œ ê¸°ì¤€ ì„¤ëª…
            with st.expander("â„¹ï¸ ì¶”ì²œ ê¸°ì¤€"):
                st.markdown("""
                **ì–´ìš¸ë¦¬ëŠ” ì°¬ì–‘ ì„ ì • ê¸°ì¤€:**
                - í˜„ì¬ ìŒì—­ëŒ€ì— ë§ëŠ” ê³¡
                - ìŒìƒ‰ê³¼ ì–´ìš¸ë¦¬ëŠ” ì¥ë¥´/ë¶„ìœ„ê¸°
                - ê°•ì ì„ ì‚´ë¦´ ìˆ˜ ìˆëŠ” í…Œí¬ë‹‰ ìš”êµ¬ì‚¬í•­

                **ë„ì „ ì°¬ì–‘ ì„ ì • ê¸°ì¤€:**
                - ì•½ì  ì˜ì—­ì„ ì—°ìŠµí•  ìˆ˜ ìˆëŠ” ê³¡
                - ì ì ˆíˆ ë„ì „ì ì´ë©´ì„œ ë¶ˆê°€ëŠ¥í•˜ì§€ ì•Šì€ ë‚œì´ë„
                - ì„±ì¥ì— ë„ì›€ì´ ë˜ëŠ” íŠ¹ì • ê¸°ìˆ  ìš”êµ¬
                """)

        with tab4:
            # MBTI ì „ì²´ íƒ€ì… íƒ­
            st.subheader("ğŸ“‹ ë³´ì»¬ MBTI ì „ì²´ ìœ í˜•")
            st.markdown("6ê°€ì§€ ë³´ì»¬ MBTI ìœ í˜•ì„ í™•ì¸í•˜ê³ , ë‹¹ì‹ ì˜ íƒ€ì…ê³¼ ë¹„êµí•´ë³´ì„¸ìš”.")

            current_type = result['primary_type']
            st.info(f"ğŸ¯ **ë‹¹ì‹ ì˜ íƒ€ì…: {current_type}**")

            st.markdown("---")

            from vocal_mbti import VOCAL_TYPES
            for code, vtype in VOCAL_TYPES.items():
                is_current = code == current_type
                icon = "âœ… " if is_current else ""
                bg_color = "background-color: #e8f5e9;" if is_current else ""

                st.markdown(f"### {icon}{code}: {vtype.name_en}")
                st.markdown(f"**{vtype.name_kr}**")
                st.write(vtype.description)

                col1, col2 = st.columns(2)
                with col1:
                    st.markdown("**âœ¨ ê°•ì :**")
                    for s in vtype.strengths:
                        st.write(f"â€¢ {s}")
                with col2:
                    st.markdown("**ğŸ¤ ë¡¤ëª¨ë¸:**")
                    for r in vtype.role_models:
                        st.write(f"â€¢ {r}")

                st.markdown("---")

        with tab5:
            # ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ íƒ­
            st.subheader("ğŸ“¥ ë¶„ë¦¬ëœ ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ")

            if hasattr(st.session_state, 'separation_result') and st.session_state.separation_result:
                sep = st.session_state.separation_result
                st.success(f"âœ… ë³´ì»¬ ë¶„ë¦¬ ì™„ë£Œ! (ì‹ ë¢°ë„: {sep['confidence'] * 100:.0f}%)")

                col1, col2 = st.columns(2)

                with col1:
                    st.markdown("### ğŸ¤ ë³´ì»¬ íŠ¸ë™")
                    st.write("ë°˜ì£¼ê°€ ì œê±°ëœ ìˆœìˆ˜ ë³´ì»¬ ìŒì„±ì…ë‹ˆë‹¤.")
                    if sep['vocals_path'] and os.path.exists(sep['vocals_path']):
                        with open(sep['vocals_path'], 'rb') as f:
                            vocals_data = f.read()
                        st.audio(vocals_data, format='audio/wav')
                        st.download_button(
                            label="ğŸ“¥ ë³´ì»¬ ë‹¤ìš´ë¡œë“œ (WAV)",
                            data=vocals_data,
                            file_name="vocals_separated.wav",
                            mime="audio/wav",
                            key="download_vocals"
                        )
                    else:
                        st.warning("ë³´ì»¬ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

                with col2:
                    st.markdown("### ğŸ¹ ë°˜ì£¼ íŠ¸ë™")
                    st.write("ë³´ì»¬ì´ ì œê±°ëœ ë°˜ì£¼(MR) ìŒì„±ì…ë‹ˆë‹¤.")
                    if sep['instrumental_path'] and os.path.exists(sep['instrumental_path']):
                        with open(sep['instrumental_path'], 'rb') as f:
                            instrumental_data = f.read()
                        st.audio(instrumental_data, format='audio/wav')
                        st.download_button(
                            label="ğŸ“¥ ë°˜ì£¼ ë‹¤ìš´ë¡œë“œ (WAV)",
                            data=instrumental_data,
                            file_name="instrumental_separated.wav",
                            mime="audio/wav",
                            key="download_instrumental"
                        )
                    else:
                        st.warning("ë°˜ì£¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

                st.markdown("---")
                st.info("ğŸ’¡ **í™œìš© íŒ:** ë¶„ë¦¬ëœ ë³´ì»¬ë¡œ ìŒì • ì—°ìŠµì„, ë°˜ì£¼ë¡œ MR ì—°ìŠµì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")

            else:
                st.info("ğŸ¤ ë³´ì»¬ ë¶„ë¦¬ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                st.write("'ë°˜ì£¼ì™€ í•¨ê»˜' ë˜ëŠ” 'ì°¬ì–‘íŒ€ê³¼ í•¨ê»˜' ì˜µì…˜ìœ¼ë¡œ ë¶„ì„í•˜ë©´ ë¶„ë¦¬ëœ ì˜¤ë””ì˜¤ë¥¼ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

        # P1: ë‹¤ìŒì— í•´ë³¼ ê²ƒ ê°€ì´ë“œ
        st.markdown("---")
        with st.expander("ğŸš€ ë‹¤ìŒì— í•´ë³¼ ê²ƒ", expanded=False):
            st.markdown("""
### ë¶„ì„ ê²°ê³¼ë¥¼ í™œìš©í•˜ëŠ” ë°©ë²•

**1ï¸âƒ£ ì´ì¤‘ ë¶„ì„ìœ¼ë¡œ ë” ê¹Šì´ ì•Œì•„ë³´ê¸°**
- ì‚¬ì´ë“œë°”ì—ì„œ 'ì´ì¤‘ ë¶„ì„' ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”
- ëŠë¦° ê³¡ + ë¹ ë¥¸ ê³¡ì„ í•¨ê»˜ ë¶„ì„í•˜ë©´ ë” ì…ì²´ì ì¸ ë³´ì»¬ í˜ë¥´ì†Œë‚˜ë¥¼ ì•Œ ìˆ˜ ìˆì–´ìš”

**2ï¸âƒ£ ì¶”ì²œ ì°¬ì–‘ ì—°ìŠµí•˜ê¸°**
- 'ì¶”ì²œ ì°¬ì–‘' íƒ­ì—ì„œ ë‹¹ì‹ ì—ê²Œ ì–´ìš¸ë¦¬ëŠ” ì°¬ì–‘ì„ í™•ì¸í•˜ì„¸ìš”
- YouTubeì—ì„œ MRì„ ì°¾ì•„ ì—°ìŠµí•´ë³´ì„¸ìš”

**3ï¸âƒ£ ì„±ì¥ í¬ì¸íŠ¸ ì—°ìŠµ**
- 'ë³´ì»¬ ì½”ì¹­' íƒ­ì˜ GROWTH POINTë¥¼ í™•ì¸í•˜ì„¸ìš”
- ê° í•­ëª©ì— ëŒ€í•œ ì—°ìŠµ ë°©ë²•ì„ ë”°ë¼í•´ë³´ì„¸ìš”

**4ï¸âƒ£ ê²°ê³¼ ê³µìœ í•˜ê¸°**
- SNS ì´ë¯¸ì§€ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ì¹œêµ¬ë“¤ê³¼ ê³µìœ í•˜ì„¸ìš”
- PDF ë¦¬í¬íŠ¸ë¡œ ì €ì¥í•˜ì—¬ ë‚˜ì¤‘ì— ë‹¤ì‹œ í™•ì¸í•  ìˆ˜ ìˆì–´ìš”

**5ï¸âƒ£ ì£¼ê¸°ì ìœ¼ë¡œ ë¶„ì„í•˜ê¸°**
- ì—°ìŠµ í›„ ë‹¤ì‹œ ë¶„ì„í•˜ì—¬ ì„±ì¥ì„ í™•ì¸í•´ë³´ì„¸ìš”
- ê°™ì€ ê³¡ì„ ì‹œê°„ ê°„ê²©ì„ ë‘ê³  ë¶„ì„í•˜ë©´ ë°œì „ ì •ë„ë¥¼ ì•Œ ìˆ˜ ìˆì–´ìš”
            """)


# =============================================
# í‘¸í„°
# =============================================

st.markdown("---")
st.markdown("Made with â¤ï¸ for Worship Leaders | Dual-Core Analysis Engine v3.0")
